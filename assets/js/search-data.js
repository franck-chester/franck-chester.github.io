var store = [{
        "title": "Yor: Automated IaC tag and trace",
        "excerpt":"This could be a useful tool ","categories": [],
        "tags": ["terraform","aws","tagging"],
        "url": "https://franck-chester.github.io//links/2021-09-21-YOR%20automated%20tagging.html"
      },{
        "title": "Distributed transaction patterns for microservices compared",
        "excerpt":"This is a good write up on distributed transaction patterns. ","categories": [],
        "tags": ["microservices","distributed-systems"],
        "url": "https://franck-chester.github.io//links/2021-09-22-distributed%20transaction%20patterns.html"
      },{
        "title": "Defining IAM Policies with Terraform safely",
        "excerpt":"Good tip on terraforming IAM policies ","categories": [],
        "tags": ["iam","terraform","infosec"],
        "url": "https://franck-chester.github.io//links/2021-09-22-terraform%20IAM%20policies.html"
      },{
        "title": "An Exploration of JSON Interoperability Vulnerabilities",
        "excerpt":"This is a great write up of json interops (or lack of) and the security consequences ","categories": [],
        "tags": ["infosec","json"],
        "url": "https://franck-chester.github.io//links/2021-09-24-json%20vulnerabilities.html"
      },{
        "title": "Fearless Salary Negotiation",
        "excerpt":"This is a great website, full of advice about salary negotiations for software developers (tho the advice seems portable to other job types).  Not sure I agree with it all but this most likely down to doing it all wrong myself.  Definitely worth a browse! Might even buy the book‚Ä¶ ","categories": [],
        "tags": ["career","negotiation"],
        "url": "https://franck-chester.github.io//links/2021-09-28-%20Fearless%20Salary%20Negotiation.html"
      },{
        "title": "Fundamental Practices for Secure Software Development",
        "excerpt":"The best practices in the guide apply to cloud-based and online services, shrink-wrapped software and database applications, as well as operating systems, mobile devices, embedded systems and devices connected to the Internet. ","categories": [],
        "tags": ["infosec"],
        "url": "https://franck-chester.github.io//links/2021-09-28-Fundamental%20Practices%20for%20Secure%20Software%20Development.html"
      },{
        "title": "Liberating structures",
        "excerpt":"This website offers an alternative way to approach and design how people work together. It provides a menu of thirty-three Liberating Structures to replace or complement conventional practices. ","categories": [],
        "tags": ["work-practices"],
        "url": "https://franck-chester.github.io//links/2021-09-28-Liberating%20structures.html"
      },{
        "title": "REST API design resource modeling",
        "excerpt":"Good article about reconciling domain driven design and REST. Quite old, but then again the DDD books I have read are as well, and couched in very old fashion SOA language, which I haven‚Äôt had much to do in recent years. Feels like both a trip down memory lane and a refresher ü§® ","categories": [],
        "tags": ["rest","ddd","architecture"],
        "url": "https://franck-chester.github.io//links/2021-09-28-REST%20API%20design%20resource%20modeling.html"
      },{
        "title": "The use of POWER for meaningful meetings",
        "excerpt":"The POWER Start is a facilitation technique developed by the Agile Coaching Institute to put an end to bad meetings. ","categories": [],
        "tags": ["work-practices"],
        "url": "https://franck-chester.github.io//links/2021-09-28-The%20use%20of%20POWER%20for%20meaningful%20meetings.html"
      },{
        "title": "developer security essentials",
        "excerpt":"Another great resource from the footnotes of ‚ÄúContinuous architecture in practice‚Äù, handbooks and training materials on ‚Äúdeveloper security essentials‚Äù. ","categories": [],
        "tags": ["infosec"],
        "url": "https://franck-chester.github.io//links/2021-09-28-developer%20security%20essentials.html"
      },{
        "title": "AWS IAM Permission Boundaries Has A Caveat That May Surprise You",
        "excerpt":"This looks interesting‚Ä¶ ","categories": [],
        "tags": ["iam"],
        "url": "https://franck-chester.github.io//links/2021-09-29-AWS%20IAM%20Permission%20Boundaries%20Has%20A%20Caveat%20That%20May%20Surprise%20You.html"
      },{
        "title": "Journey on observability ",
        "excerpt":"Great thread on observability (i11y) as a practice ","categories": [],
        "tags": ["o11y","work-practices"],
        "url": "https://franck-chester.github.io//links/2021-09-29-Journey%20on%20observability%20.html"
      },{
        "title": "Operating Lambda: Performance optimization",
        "excerpt":"This three-part series discusses performance optimization for Lambda-based applications. ","categories": [],
        "tags": ["Aws","lambda"],
        "url": "https://franck-chester.github.io//links/2021-09-29-Operating%20Lambda%20-%20Performance%20optimization.html"
      },{
        "title": "Terraform, can you keep a secret?",
        "excerpt":"Did you now terraform state can and very likely holds sensitive data? ","categories": [],
        "tags": ["terraform","IaC","infosec"],
        "url": "https://franck-chester.github.io//links/2021-10-01-Terraform-%20can%20you%20keep%20a%20secret.html"
      },{
        "title": "ugit: DIY Git in Python",
        "excerpt":"implement Git in Python to learn more about how Git works on the inside. ","categories": [],
        "tags": ["Git","python"],
        "url": "https://franck-chester.github.io//links/2021-10-03-ugit-%20DIY%20Git%20in%20Python.html"
      },{
        "title": "How Netflix uses ML to optimise thumbnails",
        "excerpt":"This could be an interesting innovation at work, for product carousels ","categories": [],
        "tags": ["Ml"],
        "url": "https://franck-chester.github.io//links/2021-10-07-How%20Netflix%20uses%20ML%20to%20optimise%20thumbnails.html"
      },{
        "title": "Fargate networking 101",
        "excerpt":"Quick overview of AWS Fargate networking ","categories": [],
        "tags": ["aws","fargate"],
        "url": "https://franck-chester.github.io//links/2021-10-08-Fargate%20networking%20101.html"
      },{
        "title": "5 patterns to make your microservice fault-tolerant",
        "excerpt":"Some good info in there about handling failure in downstream services ","categories": [],
        "tags": ["architecture","ha","resilience","microservices"],
        "url": "https://franck-chester.github.io//links/2021-10-09-5%20patterns%20to%20make%20your%20microservice%20fault-tolerant.html"
      },{
        "title": "A quick introduction to clean architecture",
        "excerpt":"Overview (or data dump) of clean architecture concepts ","categories": [],
        "tags": ["architecture"],
        "url": "https://franck-chester.github.io//links/2021-10-09-A%20quick%20introduction%20to%20clean%20architecture.html"
      },{
        "title": "AWs Caching challenges and strategies",
        "excerpt":"Tons of information about caching strategies in this AWS builders library article ","categories": [],
        "tags": ["architecture","caching","ha"],
        "url": "https://franck-chester.github.io//links/2021-10-09-AWs%20Caching%20challenges%20and%20strategies.html"
      },{
        "title": "Avoiding fallback in distributed systems",
        "excerpt":"Another information packed article from AWS builders library, advising, surprisingly, against fallback strategies ","categories": [],
        "tags": ["resilience","ha","architecture"],
        "url": "https://franck-chester.github.io//links/2021-10-09-Avoiding%20fallback%20in%20distributed%20systems.html"
      },{
        "title": "Failure Isolation and Recovery: Learning from High-Scale and Extreme-Scale Computing",
        "excerpt":"Covers a broad swath of resilience issues, with links and references to further material ","categories": [],
        "tags": ["ha","architecture"],
        "url": "https://franck-chester.github.io//links/2021-10-09-Failure%20Isolation%20and%20Recovery%20-%20Learning%20from%20High-Scale%20and%20Extreme-Scale%20Computing.html"
      },{
        "title": "GROWTH BLOG  Scalability and Technology Consulting Advice for SaaS and Technology Companies",
        "excerpt":"Scroll to the bottom to the categories. Lot of good stuff in there. ","categories": [],
        "tags": ["architecture","microservices","patterns"],
        "url": "https://franck-chester.github.io//links/2021-10-09-GROWTH%20BLOG%20%20Scalability%20and%20Technology%20Consulting%20Advice%20for%20SaaS%20and%20Technology%20Companies.html"
      },{
        "title": "Google SRE book",
        "excerpt":"TIL that Google book on Site Reliability Engineering is available online! ","categories": [],
        "tags": ["SRE","architecture","ha"],
        "url": "https://franck-chester.github.io//links/2021-10-09-Google%20SRE%20book.html"
      },{
        "title": "Hexagonal Architecture: three principles and an implementation example",
        "excerpt":"Interesting dive into hexagonal architecture, ports and adapters, dependencies always facing inward, etc‚Ä¶ ","categories": [],
        "tags": ["architecture"],
        "url": "https://franck-chester.github.io//links/2021-10-09-Hexagonal%20Architecture%20-%20%20three%20principles%20and%20an%20implementation%20example.html"
      },{
        "title": "Hexagonal architecture ",
        "excerpt":"Updated thoughts on hexagonal architecture, emphasising the importance of port and adapters to enforce strict separation of concerns between business logic and experience/presentatiin ","categories": [],
        "tags": ["Architecture"],
        "url": "https://franck-chester.github.io//links/2021-10-09-Hexagonal%20architecture%20.html"
      },{
        "title": "Seeing the BFF Pattern used in the wild",
        "excerpt":"Good overview of the BFF Pattern and its relationship with other patterns ","categories": [],
        "tags": ["bff","architecture","microservices"],
        "url": "https://franck-chester.github.io//links/2021-10-09-Seeing%20the%20BFF%20Pattern%20used%20in%20the%20wild.html"
      },{
        "title": "The API gateway pattern versus the Direct client-to-microservice communication",
        "excerpt":"Microsoft view on the API gateway and backend-for-frontend patterns ","categories": [],
        "tags": ["Microservices","architecture","api-gateway","patterns","bff"],
        "url": "https://franck-chester.github.io//links/2021-10-09-The%20API%20gateway%20pattern%20versus%20the%20Direct%20client-to-microservice%20communication.html"
      },{
        "title": "How to keep complexity in check with  hexagonal architecture",
        "excerpt":"Description of the use and advantages of hexagonal architecture in frontend components, which would include BFF. (backend for frontend), although they are not referred to as such in this article ","categories": [],
        "tags": ["bff","hexagonal-architecture","architecture"],
        "url": "https://franck-chester.github.io//links/2021-10-10-How%20to%20keep%20complexity%20in%20check%20with%20%20hexagonal%20architecture.html"
      },{
        "title": "Interview with Alistair Cockburn About Hexagonal Architecture... and more",
        "excerpt":"Always fascinating to find out where patterns originated from, from the authors themselves ","categories": [],
        "tags": ["hexagonal-architecture","patterns","architecture"],
        "url": "https://franck-chester.github.io//links/2021-10-10-Interview%20with%20Alistair%20Cockburn%20About%20Hexagonal%20Architecture/.%20and%20more.html"
      },{
        "title": "Layered Architecture: Still a Solid Approach",
        "excerpt":"There is a misconception that layered architectures equate to N-tiers architectures. They don‚Äôt and this article supports this (my) interpretation. ","categories": [],
        "tags": ["architecture","solid"],
        "url": "https://franck-chester.github.io//links/2021-10-10-Layered%20Architecture%20-%20Still%20a%20Solid%20Approach.html"
      },{
        "title": "Layering Microservices",
        "excerpt":"Another article supporting my view that layering your architecture is good, doesn‚Äôt equates to N-tiers and doesn‚Äôt clash with microservices, hexagonal and cell based architecture descriptions ","categories": [],
        "tags": ["architecture","microservices"],
        "url": "https://franck-chester.github.io//links/2021-10-10-Layering%20Microservices.html"
      },{
        "title": "The Complete History of AWS Outages",
        "excerpt":"Good read, puts things in perspective ","categories": [],
        "tags": ["aws"],
        "url": "https://franck-chester.github.io//links/2021-10-11-The%20Complete%20History%20of%20AWS%20Outages.html"
      },{
        "title": "Continuous Delivery Manifesto ",
        "excerpt":"This looks interesting‚Ä¶ ","categories": [],
        "tags": ["Devoos"],
        "url": "https://franck-chester.github.io//links/2021-10-13-Continuous%20Delivery%20Manifesto%20.html"
      },{
        "title": "Google DORA DevOps capabilities",
        "excerpt":"The DevOps Research and Assessment (DORA) team has identified and validated a set of capabilities that drive higher software delivery and organizational performance. These articles describe how to implement, improve, and measure these capabilities. ","categories": [],
        "tags": ["Devops"],
        "url": "https://franck-chester.github.io//links/2021-10-13-Google%20DORA%20DevOps%20capabilities.html"
      },{
        "title": "HelloFresh Journey to the Data Mesh",
        "excerpt":"Well written description of hello fresh transformation of their approach to data ","categories": [],
        "tags": ["Data","data-mesh","team-topologies"],
        "url": "https://franck-chester.github.io//links/2021-10-22-HelloFresh%20Journey%20to%20the%20Data%20Mesh.html"
      },{
        "title": "Pat's tech blog",
        "excerpt":"Another unassuming tech blog, written by a friend of mine. ","categories": [],
        "tags": [],
        "url": "https://franck-chester.github.io//links/2021-10-22-patgawley.html"
      },{
        "title": "Are AWS Certifications worth it in 2021?",
        "excerpt":"More research about AWS Certifications. ","categories": [],
        "tags": ["Aws-certification"],
        "url": "https://franck-chester.github.io//links/2021-10-23-Are%20AWS%20Certifications%20worth%20it%20in%202021.html"
      },{
        "title": "Tutorial dojo",
        "excerpt":"As part of my professional development, it is starting to feel like I should aim for certification.This site came up, offers allegedly good quality tutorials. ","categories": [],
        "tags": ["Aws-certification"],
        "url": "https://franck-chester.github.io//links/2021-10-23-Tutorial%20dojo.html"
      },{
        "title": "iamlive: Generate an IAM policy from AWS calls using client-side monitoring (CSM) or embedded proxy",
        "excerpt":"This could be a useful tool to identify which minimum set of action to allow on my IaC user to allow it to do its works while preserving least privilege. I have yet to find a smarter way to do this than add actions one by one, and I am not on my own - as per this terraform issue, and stackoverflow. I will, one day, experiment with iamlive, which would theoretically allow me to execute my terraform configuration with a super -user, log the corresponding access rights and then add these to my IaC user policy. ","categories": [],
        "tags": ["aws","iam"],
        "url": "https://franck-chester.github.io//links/2021-10-30-iamlive%20utility.html"
      },{
        "title": "Jeckyll plug in to create heading anchors",
        "excerpt":"Reminder to myself to add this plug in to my site to enable me to link to individual sections in posts. ","categories": [],
        "tags": ["jeckyll"],
        "url": "https://franck-chester.github.io//links/2021-11-01-Jeckyll%20plug%20in%20to%20create%20heading%20anchors.html"
      },{
        "title": "Jest testing tutorial ",
        "excerpt":"Jest is what terraform cdk uses for unit testing, so this tutorial could come handy ","categories": [],
        "tags": ["Javascript","jest","testing","cdktf"],
        "url": "https://franck-chester.github.io//links/2021-11-01-Jest%20testing%20tutorial%20.html"
      },{
        "title": "An opinionated guide on how to reverse engineer software",
        "excerpt":"Way out of my league, but very well written guide to reverse engineering. ","categories": [],
        "tags": ["Miscellaneous"],
        "url": "https://franck-chester.github.io//links/2021-11-02-An%20opinionated%20guid%20on%20how%20to%20reverse%20engineer%20software.html"
      },{
        "title": "Inframap - generate pretty terraform graphs",
        "excerpt":"Looking for something to make my CDK TF posts prettier ","categories": [],
        "tags": ["Terraform","iac"],
        "url": "https://franck-chester.github.io//links/2021-11-02-Inframap%20-%20generate%20pretty%20terraform%20graphs.html"
      },{
        "title": "Compliance in a DevOps Culture",
        "excerpt":"These pattern could become useful in the new job, streamline our compliance processes and align them with devops ","categories": [],
        "tags": ["Devops","compliance"],
        "url": "https://franck-chester.github.io//links/2021-11-03-Compliance%20in%20a%20DevOps%20Culture.html"
      },{
        "title": "DynamoDB Design Patterns for Single Table Design",
        "excerpt":"I need to up skill on dynamodb, planning to use it in a design but don‚Äôt know anywhere as much as I‚Äôd like about it. Single table design is especially interesting, seems to be a pattern worth understanding within a microservices architecture ","categories": [],
        "tags": ["DynamoDB","single-table"],
        "url": "https://franck-chester.github.io//links/2021-11-03-DynamoDB%20Design%20Patterns%20for%20Single%20Table%20Design.html"
      },{
        "title": "Exporting DynamoDB Data to Excel using Lambda with s3 on AWS",
        "excerpt":"This is something I might need to replicate for my current project,where my dynamodb is just a staging stage before the data is eventually sent to our data lake. In the meantime, emailing the data to my colleagues in marketing will do nicely üòÅ ","categories": [],
        "tags": ["dynamodb","lambda","excel","nodejs"],
        "url": "https://franck-chester.github.io//links/2021-11-03-Exporting%20DynamoDB%20Data%20to%20Excel%20using%20Lambda%20with%20s3%20on%20AWS.html"
      },{
        "title": "Safe List updates with DynamoDB",
        "excerpt":"More dynamodb stuff I should really know already ","categories": [],
        "tags": ["dynamodb"],
        "url": "https://franck-chester.github.io//links/2021-11-03-Safe%20List%20updates%20with%20DynamoDB.html"
      },{
        "title": "CountAPI This API allows you to create simple numeric counters. IaaS, Integer as a Service.",
        "excerpt":"I‚Äôm curious to see whether anyone actually visits the blog pages that I‚Äôve shared (aka attention seeking) and implementing a basic page counter would help. ","categories": [],
        "tags": ["jeckyll","javascript"],
        "url": "https://franck-chester.github.io//links/2021-11-04-CountAPI%20counter%20as%20a%20Service.html"
      },{
        "title": "How to setup a Serverless application with AWS SAM and Terraform",
        "excerpt":"Some good stuff in there. Not exactly what I‚Äôm looking for, I think, which is a strong(er) separation of infrastructure and code, but still a good read. ","categories": [],
        "tags": ["aws","devops","lambda","terraform","sam"],
        "url": "https://franck-chester.github.io//links/2021-11-05-How%20to%20setup%20a%20Serverless%20application%20with%20AWS%20SAM%20and%20Terraform.html"
      },{
        "title": "Saving Money By Replacing API Gateway With Application Load Balancer's Lambda Integration",
        "excerpt":"Our AWS support architect (whatever their title actually is) has pointed out that our planned use of API Gateway could prove quite expensive, and that,  on the face of it, the Application Load Balancer might be all we need. I am therefore doing my own research, and this article supports their argument. ","categories": [],
        "tags": ["aws","api-gateway","alb"],
        "url": "https://franck-chester.github.io//links/2021-11-05-Saving%20Money%20By%20Replacing%20API%20Gateway%20With%20Application%20Load%20Balancer%20Lambda%20Integration.html"
      },{
        "title": "DDD, Hexagonal, Onion, Clean, CQRS, ‚Ä¶ How I put it all together",
        "excerpt":"Aot to unpack in this article, saving it to read later, not convinced about everything in it but worth a proper resd ","categories": [],
        "tags": ["architecture"],
        "url": "https://franck-chester.github.io//links/2021-11-06-DDD%20Hexagonal%20Onion%20Clean%20CQRS%20...%20How%20I%20put%20it%20all%20together.html"
      },{
        "title": "A Terraform CDK Construct which doubles as native Terraform Module",
        "excerpt":"I want to see how to consume terraform modules from my CDK code. This covers the other direction, as well ","categories": [],
        "tags": ["cdktf","terraform"],
        "url": "https://franck-chester.github.io//links/2021-11-11-A%20Terraform%20CDK%20Construct%20which%20doubles%20as%20native%20Terraform%20Module.html"
      },{
        "title": "Tips to prevent a serverless wreck ",
        "excerpt":"Nice simple article pointing to some useful reference material ","categories": [],
        "tags": ["aws","serverless","o11y"],
        "url": "https://franck-chester.github.io//links/2021-11-15-Tips%20to%20prevent%20a%20serverless%20wreck%20.html"
      },{
        "title": "season of the Elastic Bytes",
        "excerpt":"Curated list of elastic observability videos ","categories": [],
        "tags": ["O11y","eks"],
        "url": "https://franck-chester.github.io//links/2021-11-15-season%20of%20the%20Elastic%20Bytes.html"
      },{
        "title": "Serverless.tf",
        "excerpt":"serverless.tf is an opinionated open-source framework for developing, building, deploying, and securing serverless applications and infrastructures on AWS using Terraform. ","categories": [],
        "tags": ["aws","terraform","serverless"],
        "url": "https://franck-chester.github.io//links/2021-11-17-Serverless.tf.html"
      },{
        "title": "A Guide to Caching with NGINX and NGINX Plus",
        "excerpt":"Useful info about using NGINX as a caching proxy or reverse-proxy ","categories": [],
        "tags": ["caching","nginx"],
        "url": "https://franck-chester.github.io//links/2021-11-18-A%20Guide%20to%20Caching%20with%20NGINX%20and%20NGINX%20Plus.html"
      },{
        "title": "Bloom is a REST API caching middleware",
        "excerpt":"I‚Äôm bothered that there are so few implementations of transparent read through caching for REST APIs. This is one, but doesn‚Äôt even use the proper cache-control headers, or asynchronous cache refresh ","categories": [],
        "tags": ["caching"],
        "url": "https://franck-chester.github.io//links/2021-11-18-Bloom%20is%20a%20REST%20API%20caching%20middleware.html"
      },{
        "title": "Cache-Control for Civilians",
        "excerpt":"More stuff about caching ","categories": [],
        "tags": ["caching","nodejs"],
        "url": "https://franck-chester.github.io//links/2021-11-18-Cache-Control%20for%20Civilians.html"
      },{
        "title": "Do not use AWS CloudFormation",
        "excerpt":"A useful comparison of Terraform and Cloudformation ","categories": [],
        "tags": ["Terraform","aws","cloudformation"],
        "url": "https://franck-chester.github.io//links/2021-11-18-Do%20not%20use%20AWS%20CloudFormation.html"
      },{
        "title": "cacheable-response",
        "excerpt":"An HTTP compliant nodejs route path middleware for serving cache response with invalidation support ","categories": [],
        "tags": ["caching","nodejs"],
        "url": "https://franck-chester.github.io//links/2021-11-18-cacheable-response.html"
      },{
        "title": "AWS Fargate: Understanding the Networking and Security Considerations",
        "excerpt":"First part of this article provides a simple overview of fargate networking ","categories": [],
        "tags": ["aws","fargate","security"],
        "url": "https://franck-chester.github.io//links/2021-11-19-AWS%20Fargate%20-%20Understanding%20the%20Networking%20and%20Security%20Considerations.html"
      },{
        "title": "ECS vs. Fargate: What's the difference?",
        "excerpt":"Things I know or think I know but fail to explain easily ","categories": [],
        "tags": ["aws","ecs","fargate","containers"],
        "url": "https://franck-chester.github.io//links/2021-11-19-ECSvsFargate.html"
      },{
        "title": "From monolith to resilient microservices",
        "excerpt":"R ","categories": [],
        "tags": ["Serverless","microservices","architecture"],
        "url": "https://franck-chester.github.io//links/2021-11-20-From%20monolith%20to%20resilient%20microservices.html"
      },{
        "title": "Ws Security Documentation by Category",
        "excerpt":"This is worth bookmarking - a list of all AWS services security pages ","categories": [],
        "tags": ["aws","security","threat-modeling"],
        "url": "https://franck-chester.github.io//links/2021-11-21-Ws%20Security%20Documentation%20by%20Category.html"
      },{
        "title": "Avoiding Premature Software Abstractions",
        "excerpt":"A good point, well maxe ","categories": [],
        "tags": ["design"],
        "url": "https://franck-chester.github.io//links/2021-11-23-Avoiding%20Premature%20Software%20Abstractions.html"
      },{
        "title": "AWS and IPv6",
        "excerpt":"Besides more IPs, what are the benefits of adopting all ipv6 and is it practical today? ","categories": [],
        "tags": ["aws","ipv6"],
        "url": "https://franck-chester.github.io//links/2021-11-25-AWS%20and%20IPv6.html"
      },{
        "title": "Cloud Native Architecture in Practice ",
        "excerpt":"Good summary of the things to look for when architecting for the cloud ","categories": [],
        "tags": ["architecture","cloud-native"],
        "url": "https://franck-chester.github.io//links/2021-11-26-Cloud%20Native%20Architecture%20in%20Practice%20.html"
      },{
        "title": "Data Mesh: An Architectural Deep Dive",
        "excerpt":"Zhamak Dehghani introduces the architecture of new Data Mesh concepts ","categories": [],
        "tags": ["datamesh"],
        "url": "https://franck-chester.github.io//links/2021-11-26-Data%20Mesh%20An%20Architectural%20Deep%20Dive.html"
      },{
        "title": "Anti-Patterns When Building Container Images",
        "excerpt":"Some great advice in here if you‚Äôre writing Dockerfiles, and some pointers for when not to! ","categories": [],
        "tags": ["ecs","docker"],
        "url": "https://franck-chester.github.io//links/2021-11-30-Anti-Patterns%20When%20Building%20Container%20Images.html"
      },{
        "title": "Serverless Container-based APIs with Amazon ECS and Amazon API Gateway",
        "excerpt":"This blog post guides you through the details of the option based on API Gateway and AWS Cloud Map, and how to implement it: first you learn how the different components (Amazon ECS, AWS Cloud Map, API Gateway, etc.) work together, then you launch and test a sample container-based API. ","categories": [],
        "tags": ["ecs","api-gateway","aws"],
        "url": "https://franck-chester.github.io//links/2021-12-03-Serverless%20Container-based%20APIs%20with%20Amazon%20ECS%20and%20Amazon%20API%20Gateway.html"
      },{
        "title": "Application Load Balancer-type Target Group for Network Load Balancer",
        "excerpt":"This new feature allows AWS customers to directly register an ALB as an NLB target, eliminating the need to actively manage changing ALB IP addresses. This is achieved by making use of a newly introduced Application Load Balancer-type target group for NLB. ","categories": [],
        "tags": ["aws"],
        "url": "https://franck-chester.github.io//links/2021-12-04-Application%20Load%20Balancer-type%20Target%20Group%20for%20Network%20Load%20Balancer.html"
      },{
        "title": "Canarytokens.org - Quick, Free, Detection for the Masses",
        "excerpt":"How to test your apps for #log4shell vulnerability 1. Generate a DNS token https://t.co/vCzVG0O03i2. Wrap that token in Prefix: ${jndi:ldap://Suffix: /a}3. Use that value in search forms, profile data, settings etc. of your apps4. Get notified when you triggered a reaction ","categories": [],
        "tags": ["Log4shell","infosec"],
        "url": "https://franck-chester.github.io//links/2021-12-10-Canarytokens.org.html"
      },{
        "title": "API interaction types in a microservice architecture: queries, commands, and events",
        "excerpt":"Nice post with interesting links ","categories": [],
        "tags": ["microservices","architecture"],
        "url": "https://franck-chester.github.io//links/2021-12-12-API%20interaction%20types%20in%20a%20microservice%20architecture.html"
      },{
        "title": "AWS Access Keys - A Reference",
        "excerpt":"AWS Access Keys are the credentials used to provide programmatic or CLI-based access to the AWS APIs. This post outlines what they are, how to identify the different types of keys, where you‚Äôre likely to find them across the different services, and the order of access precedence for the different SDKs and tools. ","categories": [],
        "tags": ["iam","aws"],
        "url": "https://franck-chester.github.io//links/2021-12-18-AWS%20Access%20Keys%20-%20A%20Reference.html"
      },{
        "title": "Aws cost control resources",
        "excerpt":"This looks interesting‚Ä¶ ","categories": [],
        "tags": ["Aws","cost"],
        "url": "https://franck-chester.github.io//links/2022-01-07-Aws%20cost%20control%20resources.html"
      },{
        "title": "AWS SLA: Are you able to keep your availability promise?",
        "excerpt":"More on AWS managed  component availability ","categories": [],
        "tags": ["Aws","ha","nines","availability"],
        "url": "https://franck-chester.github.io//links/2022-02-13-AWS%20SLA-%20Are%20you%20able%20to%20keep%20your%20availability%20promise.html"
      },{
        "title": "Uptime and downtime with 99.99 % SLA",
        "excerpt":"Was glad to note that AWS SLAs are given with monthly values, basically their invoicing period  Their SLAs vary between 3 and 4 nines, or 43m 49s to 4m 22s, per month. ","categories": [],
        "tags": ["Aws","ha","nines","availability"],
        "url": "https://franck-chester.github.io//links/2022-02-13-Uptime%20and%20downtime%20with%2099.99%20%25%20SLA.html"
      },{
        "title": "A Detailed Overview of AWS API Gateway",
        "excerpt":"Tons of useful details in this article ","categories": [],
        "tags": ["api-gateway","aws"],
        "url": "https://franck-chester.github.io//links/2022-03-09-A%20Detailed%20Overview%20of%20AWS%20API%20Gateway.html"
      },{
        "title": "Cross-account role trust policies should trust AWS accounts, not roles",
        "excerpt":"Found this in the context of access control to event carried sensitive data ","categories": [],
        "tags": ["aws","IAM"],
        "url": "https://franck-chester.github.io//links/2022-03-22-Cross-account%20role%20trust%20policies%20should%20trust%20AWS%20accounts%20not%20roles.html"
      },{
        "title": "Binbash Leverage Documentation",
        "excerpt":"A good reference reference architecture. The documentation is good as well. the leading, easy to deploy, reusable and most automated solution for defining, provisioning and managing your secure and scalable multi-account AWS infrastructure environment. ","categories": [],
        "tags": ["aws","architecture","well-architected"],
        "url": "https://franck-chester.github.io//links/2022-03-26-Binbash%20Leverage%20Documentation.html"
      },{
        "title": "Writing Logic in CSS",
        "excerpt":"Thiao many things I didn‚Äôt know about CSS ","categories": [],
        "tags": ["css"],
        "url": "https://franck-chester.github.io//links/2022-03-26-Writing%20Logic%20in%20CSS.html"
      },{
        "title": "AWS Routing 101",
        "excerpt":"Good summary of routing in aws ","categories": [],
        "tags": ["aws","vpc"],
        "url": "https://franck-chester.github.io//links/2022-03-28-AWS%20Routing%20101.html"
      },{
        "title": "Tao of Node - Design, Architecture & Best Practices",
        "excerpt":"This looks interesting‚Ä¶ ","categories": [],
        "tags": ["nodejs"],
        "url": "https://franck-chester.github.io//links/2022-03-28-Tao%20of%20Node.html"
      },{
        "title": "WASM framework - Spin",
        "excerpt":"For future reference, I‚Äôm excited about WASM in Envoy, and this could maybe help ","categories": [],
        "tags": ["wasm","enviy"],
        "url": "https://franck-chester.github.io//links/2022-04-01-WASM%20framework%20-%20Spin.html"
      },{
        "title": "AWS: 10 Things You‚Äôre Probably Doing Wrong as an Architect",
        "excerpt":"A few gotcha worth knowing about in this article ","categories": [],
        "tags": ["aws","alb","dns"],
        "url": "https://franck-chester.github.io//links/2022-04-04-AWS-10%20Things%20Youre%20Probably%20Doing%20Wrong%20as%20an%20Architect.html"
      },{
        "title": "DDD Crew",
        "excerpt":"Interesting set of resources around DDD, including some nteresting mapping tools ","categories": [],
        "tags": ["DDD"],
        "url": "https://franck-chester.github.io//links/2022-06-14-DDD%20Crew.html"
      },{
        "title": "DDD and Messaging Architectures",
        "excerpt":"TA ververy useful list of patterns ","categories": [],
        "tags": ["ddd","eda","events","patterns"],
        "url": "https://franck-chester.github.io//links/2022-07-15-DDD%20and%20Messaging%20Architectures.html"
      },{
        "title": "Lessons Learned From Running Serverless In Production",
        "excerpt":"Great blog entry feom the serverless monk ","categories": [],
        "tags": ["aws","serverless","o18y"],
        "url": "https://franck-chester.github.io//links/2022-07-15-Lessons%20Learned%20From%20Running%20Serverless%20In%20Production.html"
      },{
        "title": "Asynchronous Messaging and Eventing Resources",
        "excerpt":"Clement Vaters super list of resources about events and message driven architecture ","categories": [],
        "tags": ["events","eda"],
        "url": "https://franck-chester.github.io//links/2022-07-16-Asynchronous%20Messaging%20and%20Eventing%20Resources.html"
      },{
        "title": "Web Browser Engineering",
        "excerpt":"TThis book explains, building a basic but complete web browser, from networking to JavaScript, in a thousand lines of Python. ","categories": [],
        "tags": ["Briwser","python"],
        "url": "https://franck-chester.github.io//links/2022-07-16-Web%20Browser%20Engineering.html"
      },{
        "title": "All things FUDI2/webAuthb",
        "excerpt":"TA curated list of awesome WebAuthn/FIDO2 and now Passkey resources ","categories": [],
        "tags": ["Fidi","webauthn","infosec"],
        "url": "https://franck-chester.github.io//links/2022-08-06-All%20things%20FUDI2/webAuthb.html"
      },{
        "title": "How to choose between EventBridge and SQS in Event Driven Architecture",
        "excerpt":"This looks interesting‚Ä¶ ","categories": [],
        "tags": ["eda","sqs","eventbridge"],
        "url": "https://franck-chester.github.io//links/2022-08-09-How%20to%20choose%20between%20EventBridge%20and%20SQS%20in%20Event%20Driven%20Architecture.html"
      },{
        "title": "What is backoff for?",
        "excerpt":"This looks When do you want backoff and jitter, and when do you want adaptive retries? Are they just two ways to do the same thing, or is there something different about them? ","categories": [],
        "tags": ["resilience","patterns"],
        "url": "https://franck-chester.github.io//links/2022-08-11-What%20is%20backoff%20for?.html"
      },{
        "title": "Serverless API Essentials - Idempotency",
        "excerpt":"Great write-up on API Idempotency ","categories": [],
        "tags": ["api","Idempotency"],
        "url": "https://franck-chester.github.io//links/2022-09-14-Serverless%20API%20Essentials%20-%20Idempotency.html"
      },{
        "title": "Skip The Lambda Function, Connect Directly To Your AWS Services",
        "excerpt":"Thhow to call aws services directly from api gateway ","categories": [],
        "tags": ["serverless"],
        "url": "https://franck-chester.github.io//links/2022-10-15-Skip%20The%20Lambda%20Function,%20Connect%20Directly%20To%20Your%20AWS%20Services.html"
      },{
        "title": "SLSA dip ‚Äî At the Source of the problem!",
        "excerpt":"This article is part of a series about the security of the software supply chain. Each article will be analyzing a component of the Supply chain Levels for Software Artifacts (SLSA) model in depth, from the developer‚Äôs workstation all the way to the consumer side of the chain. ","categories": [],
        "tags": ["Infosec","sbom"],
        "url": "https://franck-chester.github.io//links/2022-11-15-SLSA%20dip%20%E2%80%94%20At%20the%20Source%20of%20the%20problem!.html"
      },{
        "title": "DDD, Hexagonal, Onion, Clean, CQRS, ‚Ä¶ How I put it all together",
        "excerpt":"This looks interesting‚Ä¶ ","categories": [],
        "tags": ["Architecture","cqrs","hexagonal"],
        "url": "https://franck-chester.github.io//links/2022-12-03-DDD,%20Hexagonal,%20Onion,%20Clean,%20CQRS,%20...%20How%20I%20put%20it%20all%20together.html"
      },{
        "title": "Setting up my GitHub page with Jekyll",
        "excerpt":"I have decided to finally set up a mini blog type site on my under utilised GitHub.The main intention is to have a single place to put all the random stuff I find when browsing the internet, twitter and linkedin, in a format I can then share as and when I think it could be useful to the wider world. GitHub Pages I follow the instructions to setup a new repository franck-chester.github.io on my personal GitHub https://github.com/franck-chester. My default branch, where I will create content and configure Jekyll, is main.I create an additional gh-pages branch where I will publish the built up site.I then configure GitHub pages to source content from gh-pages Finally, I clone this blank repository to my ubuntu environment on WSL2. Jekyll Jekyll is a static site generator, and comes recommended when setting up GitHub Pages. Setting it up in my new repository was no problem, just follow the instructions. It isn‚Äôt officially supported on Windows, but not an issue here as I have just setup Ubuntu on WSL2.Plus running it locally on Ubuntu keeps it all much closer to the GitHub action environment that I intend to use for the day to day site generation. Look &amp; Feel - So Simple I don‚Äôt really want to spend too much time styling this, so will simply use one of the many available free Jekyll themes, in this case So Simple looks good enough - as per this example site. Configuring So-Simple From the README.ME file: Add this line to your Jekyll site‚Äôs Gemfile (or create one): gem \"jekyll-theme-so-simple\"Add this line to your Jekyll site‚Äôs _config.yml file: theme: jekyll-theme-so-simpleThen run Bundler to install the theme gem and dependencies: bundle installCreate some default content In the root of my site, I create the following pages, using markdown, and the corresponding So-Simple layouts in the front-matter :   index.md  posts.md  search.md  tags.mdConfiguring GitHub We can‚Äôt use GitHub Pages‚Äô built in Jekyll functionality, as it is many versions behind and limits the gems we can use.Instead, we will use helaili/jekyll-action to publish our GitHub pages via a GitHub action. Again, we can simply use the documentation, making sure to target the gh-pages branch. We can push our main branch to GitHub, which triggers the GitHub action, which results in a nicely styled blog like site - result. Development cycle The easiest IMO is to launch vs code from within WSL2 code . and use it to both edit the content and launch a local Jekyll instance with bundle exec jekyll serve --drafts --futureI can test the pages at http://127.0.0.1:4000 before I commit to GitHub. Also, use this when adding plugins : gem update","categories": [],
        "tags": ["jekyll","github"],
        "url": "https://franck-chester.github.io//2021/09/27/jekyll.html"
      },{
        "title": "Customising Jekyll",
        "excerpt":"Following my initial setup of Jekyll (as per Setting up my GitHub page with Jekyll), I now want to customise the site to meet my personal requirements. First of all, I do not intend to blog daily, not really my style. I am however a prolific internet browser, and like to share interesting links I stumble about. I do not like cross-posting, therefore I usually have to chose whether to share on twitter, linkedin or my current job‚Äôs intranet. This means that 1) I might not reach the right audience and 2) I later cannot find these interesting links when their subject matter becomes relevant to the job at hand. I therefore want to use this site to curate links, and make it as easy as possible for me to write and publish the information, ideally as easy as it would be when I share to social media. Links will be managed as a separate Jekyll collection so that:   they do not appear in the main feed, as it would dilute it and make it boring  get styled differently from posts, mainly to use front-matter custom variables to define the link, giving it a title, a url, a source and some optional descriptive content (although tbf, this is usually limited to ‚Äòhey, this is interesting‚Äô)Second, I am keen on tagging content. Again, this is to allow me to find my own material at a later date. Now, unfortunately, Jekyll is not consistent in how it exposes tagged content, with site.tags only returning the tags used on posts, while collections need to be iterated separately to find the tags.I will therefore need to customise (aka hack) So-Solid‚Äôs tags layout to include tagged links. Finally, I want to emulate a colleague of mine (hi Paul!) who publishes a weekly summary of his reading/viewing/listening to our colleagues, which is a much better way to do it than my wanton posting on random intranet channels.I will therefore create yet another collection, weekending, with its own custom layout, to group posts and links per week, with an optional blurb should I want to expand on my tech activities that week. Update March 22: I wasn‚Äôt posting often enough to generate any useful weekly summaries, so have now deleted this section of the site. Collections As per the instructions, add this to my _config.yaml: collections:  thisWeek:    output: true  links:    output: trueand this, to apply default layouts: defaults:  -     scope:      path: \"_thisWeek\"      layout: posts    values:      strip_title: true  -    scope:      path: \"_links\"       type: \"links\"     values:      layout: linkstrip_title: true is used with the jekyll-titles-from-headings plugin to ensure that So-Simple doesn‚Äôt display the collection name as well as the actual title. Link layout Is a cut and paste from the So-Solid post.html layout (to display a single post), hacked to display the link variables:   target: the actual URL I want to share  title: a short description of the link  source: where/how I found that link in the first place (free text)  source_url: where/how I found that link in the first place (URL)  tags: keywords associated with this linkSee the result Week Ending layout Another cut and paste, this time from the So-Solid  posts.html layout (to display multiple posts), hacked so that:   it displays a standard title of ‚ÄòWeek ending‚Äô and the front-matter variable date  it displays posts AND links, under separate headings  it filter these based on their date being within 7 days of the front-matter variable dateThe main difficulty here was filtering on dates. The trick is to use the capture tag to create time variables in unix format (number of seconds) which can then be manipulated as integers: {% capture weekending %}{{page.date | date: \"%s\" }}{% endcapture %}{% capture seven_days_ago %}{{weekending |minus: 604800}}{% endcapture %}{% capture date %}{{entry.date | date: '%s'}}{% endcapture %}{% if seven_days_ago &lt; date  and date &lt;= weekending %}  ... show the post or link ...{% endif %}See the result Tags layout Another cut and paste, this time from the So-Solid tags.html layout (to display posts grouped by tag), hacked so that it considers tagged posts AND links. The hardest bit here was to deal with Liquid arrays. An initial google of the issue led to all sort of outdated advice, until I figured out that Jekyll version of Liquid provides additional array filters that make it slightly easier to manipulate them. To initialise an empty array, you still need to split an empty string, but you can then use the push filter to add entries to it: {% assign items = \"\" | split: ',' %}{% assign items = \"\" | push: 'a value' %}That said, after a lot of faffing, I simply filtered down tag collections before concatenating them: {% assign taggedItems = \"\" | split: ',' %}{% assign taggedItems = taggedItems  | concat: site.posts| where_exp : \"post\", \"post.tags contains tag\" %}{% assign taggedItems = taggedItems  | concat: site.links | where_exp : \"link\", \"link.tags contains tag\" %}What is really pants is that Liquid doesn‚Äôt give us access to Map objects, other than through the group_by filter.My logic therefore is forced to iterate through all tagged content multiple time in order to replicate the logic of the original layout, which itself could do with the built-in site.tags map, keyed on tag names. Includes So-Solid makes heavy use of Jekyll includes.Unfortunately, these stopped me making a simple cut and paste of the So-Solid‚Äôs layouts, without cut and pasting the entire include folder into mine.I will, later, try and see if I could reference the file within the gemfile.I have also initially made a mess of things by cut and pasting the markup from these files into mines, which will lead to duplication and might stop me from picking up bug fixes in later releases of So-Solid. I will tidy this up later. Debugging Liquid Worth remembering this trick : you can use Jekyll inspect filter to dump the value of any variable: myVariable = {{ myVariable| inspect }} ","categories": [],
        "tags": ["jekyll","github","liquid"],
        "url": "https://franck-chester.github.io//2021/09/28/customising-jekyll.html"
      },{
        "title": "Generate Jekyll pages with GitHub actions",
        "excerpt":"Now that I have customised Jekyll I want to simplify my workflow. Although I am starting this with a bunch of actual posts, most of my content will be links. I usually share these as soon as I‚Äôve followed the link and read the content, usually straight from my phone while browsing in the evening. I therefore need the ability to generate links entries on my site as easily as possible, from my phone. My plan is to use GitHub actions with a manual trigger to generate the markdown files, with action inputs matched to the required front-matter variables. Generate files with GitHub actions First of all, what do I need to generate files in my repository from a GitHub action? We‚Äôll the Write File Action, as echoing echo each line in a run step looks like a pain. The content of the file is easier to define as a multiline string, indicated wth a pipe |. I also need to set the date in the filename. I‚Äôll do it as per this SO answer, which uses the ::set-output:: workflow command. Finally, we need to commit the file back to main.Unfortunately this doesn‚Äôt trigger our jekyll build action, as explained here :sad: At this stage I have not figured out why using a personal token hasn‚Äôt worked around this issue‚Ä¶. To be continued Plan B is to use a repository_dispatch event trigger as per this article. Which works!! Final workflow definition looks like this (check the repo for the latest definition) name: Generate a new file in the _links folderon:  workflow_dispatch:    inputs:      title:        description: 'A user friendly name/description for the URL we are sharing'             required: true      target:        description: 'The URL we are sharing'             required: true        default: ''      source:        description: 'Where (free text) did we find out about the URL we are sharing'             required: true        default: 'the internet'      source_url:        description: 'Where (URL) did we find out about the URL we are sharing'             required: false      tags:        description: 'Space separated list of tags'             required: true      blurb:        description: 'Some text to explain why you are sharing'             required: true        default: 'This looks interesting...'jobs:  newlink:    runs-on: ubuntu-latest    steps:      - name: Clone repository        uses: actions/checkout@v2      - name: Generate the filename        id: filename        run: |          date=\"$(date +'%Y-%m-%d')\"          filename=\"${date}-$.md\"          echo \"filename = ${filename}\"          echo \"::set-output name=filename::${filename}\"      - name: Generate new link file        uses: DamianReeves/write-file-action@v1.0        with:          path: _links/$          write-mode: append          contents: |            ---            title: $            target : $            source : $            source_url : $            tags: $            ---            $      - name: Commit and push files        run: |          git config user.name \"NewLink GitHub Action\"          git config user.email \"&lt;&gt;\"          git add _links/$          git commit -m \"New link: $\"          git push https://$@github.com/$.git main      - name: Trigger jekyll build        run: |          curl -X POST https://api.github.com/repos/$/dispatches \\          -H 'Accept: application/vnd.github.everest-preview+json' \\          -u $ \\          --data '{\"event_type\": \"New link: $\", \"client_payload\": { \"customField\": \"customValue\" }}'","categories": [],
        "tags": ["github","jekyll"],
        "url": "https://franck-chester.github.io//2021/09/29/Generate-Jekyll-pages-with-GitHub-actions.html"
      },{
        "title": "Installing Podman on WSL2",
        "excerpt":"As I have a new laptop to go with the new job I am setting up my usual toolset on it. One thing I need is the ability to run containers locally, ideally grouped via docker-compose as I have a few personal projects that already use it. Unfortunately, Docker Desktop, is no longer free to use and requires a licence for large enterprises like Very. I quick google search leads me to this article on running docker on WSL without Docker Desktop. The article itself indicates that using podman would be a nice and maybe easier alternative, as Podman doesn‚Äôt require a daemon, whereas docker needs systemd and WSL2 doesn‚Äôt have systemd out of the box. Podman can now also run docker-compose files, therefore I am going to try that first and follow this other article by the same author : Using podman instead of docker on Windows Subsystem for Linux (WSL 2) Install WSL2 Using these instructions. Upgrade Ubuntu to 21.04 Unfortunately, the WSL2 install will only deploy the latest LTS version of the corresponding Linux distribution.For Ubuntu, this is 20.04 LTS which was published in April (04) 2020. However, the Podman installation package is included in the native repositories of Ubuntu since 20.10, and the latest (tho not LTS) version is Ubuntu 21.04 (Hirsute Hippo). As this is meant to be my sandbox for learning, I might as well upgrade to 21.04. I‚Äôll use these instructions.However, these hit an undocumented snag, once again caused by the absence of systemd in WSL.The explanation and the fix for this is documented here. Install Podman I follow the official instructions, which, now that I am running on 21.04, consist of: sudo apt-get -y updatesudo apt-get -y install podmanNB: on Ubuntu, we need to add docker.io as a default search registry in order to pull containers by their short name (see Shortnames are broken in 3.0.0 due to missing list of unqualified-search registries ), by adding this line to /etc/containers/registries.conf: unqualified-search-registries=[\"docker.io\"]Alias vs-code Thanks to Visual Studio Code (vscode) Remote - WSL extension, I can launch my favourite IDE from WSL2 command line: code .. Actually I can‚Äôt because I run Visual Studio Code (vscode) Insider Build coz I‚Äôm a software architect, code mostly for fun and can therefore live with a potentially unstable environment.The executable for this is an unyieldy code-insider. No problem, I just need to alias the command, which I barely remembered how to do :grin: : edit  ~/.bashrc and add a line that reads alias code='code-insiders'. Install Git WSL basically installs its own file system, therefore, when developing in WSL2, I need a separate workspace and a separate instance of git. Follow these instructions, in particular the bit about the credential manager. And that‚Äôs me done‚Ä¶ I actually haven‚Äôt run any container yet, I‚Äôll need to resurrect some old project of mine, or start a new one‚Ä¶ ","categories": [],
        "tags": ["podman","docker","containers","wsl2","ubuntu"],
        "url": "https://franck-chester.github.io//2021/09/30/Installing-podman-on-WSL2.html"
      },{
        "title": "Scripting access to my AWS sandbox",
        "excerpt":"Today I am documenting how I intend to use the AWS sandbox environments provided by my current employer. These are phoenix environments - we can book them for anything between 1 and 7 days, after which time they get wiped out with AWS nuke.We get given near free rein, with AdministratorAccess  AWS managed policy. We are also supposedly restricted in the EC2 instances size we can launch, although I have not seen this reflected in the policies attached to my user :confused: . If you‚Äôve stumbled upon these instructions from somewhere else, you might have been given an account by your employer, or more likely, as I had to do in my previous job, used your own credit card to get one and hope the AWS free tier is enough for you to play with. NB: I use a windows machine and the powershell terminal. Pre-reqs AWS CLI Install the AWS command line interface, v2, as per Installing, updating, and uninstalling the AWS CLI version 2 on Windows &gt; msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi&gt; aws --versionaws-cli/2.2.38 Python/3.8.8 Windows/10 exe/AMD64 prompt/offAWS powershell tools This is mostly to manage my iac profile and credentials, as I prefer to use the cli directly. However, the Set-AWSCredentials cmdlet will be used in my scripts to manage my cli profile. Follow the instructions for Installing the AWS tools for powershell on windows, limiting ourselves to the AWS.Tools.Common module. Install-Module -Name AWS.Tools.InstallerInstall-AWSToolsModule AWS.Tools.Common -CleanUpBook a sandbox This is a process internal to my current employer. They provide us with a portal that triggers a script that initialises a dedicated AWS account with a set of admin credentials, before emailing the user (that is me) with the account details and a one-time password: Hello, franck@acme.comPlease find your login details below:Console: https://012345678910.signin.aws.amazon.com/console/Username: franck@acme.comPassword: 7w9cu61Tl8T0EXAMPLE Create access key for the admin user The first order of business is to logon to the console to create an access key for my admin user.The idea is that once I have done this, I will not use the console to manage my sandbox, but do everything via Command Line Interface (CLI) and Infrastructure as Code (IaC) scripts. The admin user password must be changed on first access. I use a password I manage in my keepass password manager. I then navigate to the IAM (identity and Access Management) web console to create an access key. I either keep the page up while I perform the next step, or cut and paste the key details somewhere safe. Create or Update my admin-sandbox profile Configure a named profile as per Quick configuration with aws configure. Once the admin-sandbox profile has been created once, the existing values can be reused by simply pressing return when prompted &gt; aws configure --profile admin-sandboxAWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLEAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEYDefault region name [None]: eu-west-2Default output format [None]: jsonInitialise an IaC specific profile Only need to do this once, I‚Äôll reuse the one profile across my sandbox experiments. &gt; aws configure --profile franck-iacAWS Access Key ID [None]: AWS Secret Access Key [None]: Default region name [None]: eu-west-2Default output format [None]: jsoncreate policy via CLI I am keen on limiting my IaC user to the permissions needed for the specific experiment I am running, and nothing more. Each experiment will define the IaC policy as a JSON document, for example: {    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": [                \"ec2:DescribeImages\",                \"es:*\",                \"ec2:CreateImage\"            ],            \"Resource\": \"*\"        }    ]}NB: very bad example, at this stage I do not have a policy document I have actually used! I can then use this file with the CLI create-policy: &gt; aws iam create-policy --profile admin-sandbox --policy-name franck-iac --policy-document file://iac-policy.json --tag Key=sandbox,Value=franck-iac{    \"Policy\": {        \"PolicyName\": \"franck-iac\",        \"PolicyId\": \"ANPAQSHUG5ELHLEXAMPLE\",        \"Arn\": \"arn:aws:iam::012345678910:policy/franck-iac\",        \"Path\": \"/\",        \"DefaultVersionId\": \"v1\",        \"AttachmentCount\": 0,        \"PermissionsBoundaryUsageCount\": 0,        \"IsAttachable\": true,        \"CreateDate\": \"2021-09-30T07:00:06+00:00\",        \"UpdateDate\": \"2021-09-30T07:00:06+00:00\",        \"Tags\": [            {                \"Key\": \"sandbox\",                \"Value\": \"franck-iac\"            }        ]    }}If and when I need to update this policy to gradually add the required access right, I will update the file and call create-policy-version: &gt; aws iam --profile admin-sandbox create-policy-version --set-as-default --policy-arn arn:aws:iam::012345678910:policy/franck-iac --policy-document file://iac-policy.json{    \"PolicyVersion\": {        \"VersionId\": \"v2\",        \"IsDefaultVersion\": true,        \"CreateDate\": \"2021-10-29T13:22:09+00:00\"    }}The --set-as-default is important, without it our IaC user will not be attached to the new version! NB: A managed policy can have up to 5 versions. Before you create a new version, we must delete an existing version (delete-policy-version). Here we don‚Äôt really care, therefore we‚Äôll always delete the previous version. To script this, we‚Äôll use list-policy-versions and delete anything with ` ‚ÄúIsDefaultVersion‚Äù: false`. Create IaC user via CLI I use create-user: &gt; aws iam create-user --profile admin-sandbox --user-name franck-iac --permissions-boundary arn:aws:iam::012345678910:policy/franck-iac --tag Key=sandbox,Value=franck-iac{    \"User\": {        \"Path\": \"/\",        \"UserName\": \"franck-iac\",        \"UserId\": \"AIDAQSHUG5ELIKEXAMPLE\",        \"Arn\": \"arn:aws:iam::012345678910:user/franck-iac\",        \"CreateDate\": \"2021-09-30T07:05:43+00:00\",        \"PermissionsBoundary\": {            \"PermissionsBoundaryType\": \"Policy\",            \"PermissionsBoundaryArn\": \"arn:aws:iam::012345678910:policy/franck-iac\"        },        \"Tags\": [            {                \"Key\": \"sandbox\",                \"Value\": \"franck-iac\"            }        ]    }}Attach the policy to the user Now, do note that the previous command set the user‚Äôs permission boundaries, that is the maximum range of what it is allowed to do, which is different from its actual permissions. These are set separately, with attach-user-policy. Here I am being either silly and/or paranoid, as I use the same policy document for boundaries and actual rights. This will guarantee that the policy is the one and only definition of what this user can do. aws iam attach-user-policy --profile admin-sandbox --user-name franck-iac --policy-arn arn:aws:iam::012345678910:policy/franck-iac --tag Key=sandbox,Value=franck-iac Create access key for user I use create-access-key: aws iam create-access-key --profile admin-sandbox  --user-name franck-iac{    \"AccessKey\": {        \"UserName\": \"franck-iac\",        \"AccessKeyId\": \"AKIAQSHUG5ELFEXAMPLE\",        \"Status\": \"Active\",        \"SecretAccessKey\": \"ZVtmI0yh0J0Z+NhfTsIVl7Ell4PelOiXGEXAMPLE\",        \"CreateDate\": \"2021-09-30T07:07:48+00:00\"    }}Save access keys to IaC user profile This is where we use the Set-AWSCredentials. Worth noting that on platforms that support the encrypted credential file the profile is written to the encrypted store. If the platform does not support the encrypted store (Linux, MacOS, Windows Nano Server) the profile is written to the plain text ini-format shared credential file at %HOME%.aws\\credentials. To force the profile to be written to the shared credential file on systems that support both stores (i.e. Windows), specify the path and filename of the credential file using the -ProfileLocation parameter. Our command is therefore Set-AWSCredential -AccessKey AKIAQSHUG5ELFEXAMPLE -SecretKey ZVtmI0yh0J0Z+NhfTsIVl7Ell4PelOiXGEXAMPLE -StoreAs franck-iac -ProfileLocation \"$($env:userprofile)\\.aws\\credentials\" At this point I now have a IaC profile I will be able to use with Terraform. Tearing things down Although the environment will eventually be nuked, I am keen to do the right thing and clean up after myself. I will therefore generate cli-input.json files which I‚Äôll save locally so that they can be used the delete the policy, access key and user we‚Äôve just created Handle errors in my scripts See Understanding return codes from the AWS CLI Putting it all together variables.ps1 $username = 'franck-iac'$sandbox = 'franck-iac'$tags = \"Key=sandbox,Value=$sandbox\" $identity = aws sts get-caller-identity --profile admin-sandbox | ConvertFrom-Json $account = $identity.Accountsetup.ps1 .\"$PSSCriptRoot/variables.ps1\"write-host \"Create access policy for IaC users ...\"$policy = aws iam create-policy --profile admin-sandbox --policy-name $username --policy-document file://$PSScriptRoot/iac-policy.json --tag $tagsif($lastexitcode -eq 254){    # The command returned an error, probably because the policy already exists    ## for example if we ran this script multiple time    # Recreate the arn from first principle (ie the account id and polic name    $PolicyArn = \"arn:aws:iam::$($account):policy/$($username)\"    write-host \"Attempt to retrieve existing access policy for IaC users ...\"    $policy = aws iam get-policy --policy-arn $PolicyArn}if($lastexitcode){    write-error \"Aborting script, unable to create or retrieve access policy\"    exit -1}$ap =  $policy | ConvertFrom-Json  $policyArn = $ap.Policy.Arnwrite-host \"Created access policy for IaC users, ARN = $policyArn\"write-host \"Create IaC user '$username' ...\"$u = aws iam create-user --profile admin-sandbox --user-name $username --permissions-boundary $policyArn --tag $tags | ConvertFrom-Json  write-host \"Created IaC user '$username', ARN = $($u.User.Arn)\"write-host \"Attach policy $policyArn to user '$username'...\"aws iam attach-user-policy --profile admin-sandbox --user-name $username --policy-arn $policyArn --tag $tags write-host \"Create access key for IaC user '$username' ...\"$ak = aws iam create-access-key --profile admin-sandbox --user-name $username | ConvertFrom-Json  $accesskeyId = $ak.AccessKey.AccessKeyIdwrite-host \"Create cli-input file for eventual call to delete-access-key when we tear this down......\"@\"{    \"UserName\": \"$username\",    \"AccessKeyId\": \"$accesskeyId\"}\"@ | set-content \"$PSScriptRoot/cli-input-delete-access-key.json\"write-host \"Created access key for IaC user '$username', ID = $accesskeyId\"write-host \"Update profile for IaC user '$username' ...\"Set-AWSCredential -AccessKey $accesskeyId -SecretKey $ak.AccessKey.SecretAccesskey -StoreAs $username -ProfileLocation \"$($env:userprofile)\\.aws\\credentials\" write-host \"Updated profile for IaC user '$username' . \"Write-host \"All done - you can now terraform at will\"update-policy.ps1 This is the script I run everytime I need to tweak my IaC user access right by editing iac-policy.json: .\"$PSSCriptRoot/variables.ps1\"# Recreate the arn from first principle (ie the account id and polic name$PolicyArn = \"arn:aws:iam::$($account):policy/$($username)\"write-host \"Create new default version of policy '$username' / $PolicyArn...\"$pv = aws iam create-policy-version --profile admin-sandbox --set-as-default --policy-document file://iac-policy.json --policy-arn $PolicyArn | ConvertFrom-Json  if(-not $lastexitcode){    write-host \"Created version $($pv.PolicyVersion.VersionId) of policy '$username'\"}write-host \"Delete previous versions of policy '$username' ...\"$pl = aws iam list-policy-versions --profile admin-sandbox --policy-arn $PolicyArn | ConvertFrom-Json  foreach ($v in $pl.Versions) {    if (-not $v.IsDefaultVersion) {        write-host \"Delete version $($v.VersionId) of access policy '$username' ...\"        aws iam delete-policy-version  --profile admin-sandbox --policy-arn $PolicyArn --version-id $v.VersionId    }}write-host \"Delete previous versions of policy '$username': done\"teardown.ps1 The teardown script is all about undoing everything `setup.ps1‚Äô did, in reverse order.The order matters, you cannot delete a policy that is attached to a user. .\"$PSSCriptRoot/variables.ps1\"write-host \"Delete Access Key for IaC user '$username' ...\"aws iam delete-access-key  --profile admin-sandbox --cli-input-json file://$PSScriptRoot/cli-input-delete-access-key.jsonwrite-host \"Detach policy $policyArn to user '$username'...\"# Recreate the arn from first principle (ie the account id and polic name$PolicyArn = \"arn:aws:iam::$($account):policy/$($username)\"aws iam detach-user-policy --profile admin-sandbox --user-name $username --policy-arn $policyArnwrite-host \"Delete IaC user '$username' ...\"aws iam delete-user  --profile admin-sandbox --user-name $username write-host \"Delete access policy '$username' ...\"aws iam delete-policy --profile admin-sandbox --policy-arn $PolicyArnwrite-host \"Sandbox IaC user teardown complete ...\"What next? I‚Äôll want to invoke get-cost-and-usage to get an idea of the overall cost of my experiment. I‚Äôll also want to parametrise the scripts to allow me to pass the policy file name as an argument, allow me to have experiment specific policies. That said, the next post in this series will be about me using the Terraform CDK to setup infrastructure in the sandbox. ","categories": [],
        "tags": ["aws-cli","sandbox","iac"],
        "url": "https://franck-chester.github.io//2021/10/01/Scripting-access-to-my-AWS-sandbox.html"
      },{
        "title": "Configuring Git to use the proper SSH key across multiple remote repositories",
        "excerpt":"The new job uses both GitLab and BitBucket, and mandates SSH to access both. Turns out this requires additional steps that weren‚Äôt documented. After setting up SSH for bitbucket, my SSH connection was fine: PS C:\\_workspaces&gt; ssh-add -l3072 SHA256:XBzl9AbCdEfGhIjKlMnOPCm3VfP9Z9M9/iB8dG7dw24 acme\\franck@HALL-9001 (RSA)PS C:\\_workspaces&gt; ssh -T git@bitbucket.orgauthenticated via ssh key.You can use git to connect to Bitbucket. Shell access is disabledBut I was actually unable to clone a repo: PS C:\\_workspaces&gt; git clone git@bitbucket.org:acme/emar-superduperapp_ios.gitCloning into 'emar-superduperapp_ios'...git@bitbucket.org: Permission denied (publickey).fatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists.First of all, what to do when git misbehave? Turn on tracing: See this SO, How can I debug git/git-shell related problems? and the official doc Here, the option that helped was to turn SSH tracing on: $Env:GIT_SSH_COMMAND=\"ssh -v\"I can now compare what happens when I SSH directly: PS C:\\_workspaces&gt; ssh -vT git@bitbucket.orgOpenSSH_for_Windows_8.1p1, LibreSSL 3.0.2debug1: Connecting to bitbucket.org [104.192.141.1] port 22.debug1: Connection established.debug1: identity file C:\\\\Users\\\\franck/.ssh/id_rsa type 0debug1: identity file C:\\\\Users\\\\franck/.ssh/id_rsa-cert type -1debug1: identity file C:\\\\Users\\\\franck/.ssh/id_dsa type -1debug1: identity file C:\\\\Users\\\\franck/.ssh/id_dsa-cert type -1debug1: identity file C:\\\\Users\\\\franck/.ssh/id_ecdsa type -1debug1: identity file C:\\\\Users\\\\franck/.ssh/id_ecdsa-cert type -1debug1: identity file C:\\\\Users\\\\franck/.ssh/id_ed25519 type -1debug1: identity file C:\\\\Users\\\\franck/.ssh/id_ed25519-cert type -1debug1: identity file C:\\\\Users\\\\franck/.ssh/id_xmss type -1debug1: identity file C:\\\\Users\\\\franck/.ssh/id_xmss-cert type -1debug1: Local version string SSH-2.0-OpenSSH_for_Windows_8.1debug1: Remote protocol version 2.0, remote software version conker_df6142773d 0c1acfdf8d93debug1: no match: conker_df6142773d 0c1acfdf8d93debug1: Authenticating to bitbucket.org:22 as 'git'debug1: SSH2_MSG_KEXINIT sentdebug1: SSH2_MSG_KEXINIT receiveddebug1: kex: algorithm: curve25519-sha256@libssh.orgdebug1: kex: host key algorithm: ssh-rsadebug1: kex: server-&gt;client cipher: chacha20-poly1305@openssh.com MAC: &lt;implicit&gt; compression: nonedebug1: kex: client-&gt;server cipher: chacha20-poly1305@openssh.com MAC: &lt;implicit&gt; compression: nonedebug1: expecting SSH2_MSG_KEX_ECDH_REPLYdebug1: Server host key: ssh-rsa SHA256:zzZyXwVuTsRqPoNmLkkJYKwbHaxvSc0ojez9YXaGp1Adebug1: Host 'bitbucket.org' is known and matches the RSA host key.debug1: Found key in C:\\\\Users\\\\franck/.ssh/known_hosts:4debug1: rekey out after 134217728 blocksdebug1: SSH2_MSG_NEWKEYS sentdebug1: expecting SSH2_MSG_NEWKEYSdebug1: SSH2_MSG_NEWKEYS receiveddebug1: rekey in after 134217728 blocksdebug1: Will attempt key: acme\\\\franck@HALL-9001 RSA SHA256:XBzl9AbCdEfGhIjKlMnOPCm3VfP9Z9M9/iB8dG7dw24 agentdebug1: Will attempt key: C:\\\\Users\\\\franck/.ssh/id_rsa RSA SHA256:WvJsAbCdEfGhIjKlMnOP0YBFy1XWm2/P6jCL7cBmQU4debug1: Will attempt key: C:\\\\Users\\\\franck/.ssh/id_dsadebug1: Will attempt key: C:\\\\Users\\\\franck/.ssh/id_ecdsadebug1: Will attempt key: C:\\\\Users\\\\franck/.ssh/id_ed25519debug1: Will attempt key: C:\\\\Users\\\\franck/.ssh/id_xmssdebug1: SSH2_MSG_SERVICE_ACCEPT receiveddebug1: Authentications that can continue: publickeydebug1: Next authentication method: publickeydebug1: Offering public key: acme\\\\franck@HALL-9001 RSA SHA256:XBzl9AbCdEfGhIjKlMnOPCm3VfP9Z9M9/iB8dG7dw24 agentdebug1: Server accepts key: acme\\\\franck@HALL-9001 RSA SHA256:XBzl9AbCdEfGhIjKlMnOPCm3VfP9Z9M9/iB8dG7dw24 agentdebug1: Authentication succeeded (publickey).Authenticated to bitbucket.org ([104.192.141.1]:22).debug1: channel 0: new [client-session]debug1: Entering interactive session.debug1: pledge: networkdebug1: client_input_channel_req: channel 0 rtype exit-status reply 0authenticated via ssh key.You can use git to connect to Bitbucket. Shell access is disableddebug1: channel 0: free: client-session, nchannels 1Transferred: sent 3008, received 1912 bytes, in 0.2 seconds.0To what happens when I connect via git: PS C:\\_workspaces&gt; $Env:GIT_SSH_COMMAND=\"ssh -v\"PS C:\\_workspaces&gt; git clone git@bitbucket.org:acme/emar-superduperapp_ios.git11:00:04.773743 exec-cmd.c:237          trace: resolved executable dir: C:/Program Files/Git/mingw64/bin11:00:04.776745 git.c:455               trace: built-in: git clone git@bitbucket.org:acme/emar-superduperapp_ios.gitCloning into 'emar-superduperapp_ios'...11:00:04.940743 run-command.c:666       trace: run_command: unset GIT_DIR; GIT_PROTOCOL=version=2 'ssh -v' -o SendEnv=GIT_PROTOCOL git@bitbucket.org 'git-upload-pack '\\''acme/emar-superduperapp_ios.git'\\'''OpenSSH_8.7p1, OpenSSL 1.1.1k  25 Mar 2021debug1: Reading configuration data /etc/ssh/ssh_configdebug1: Connecting to bitbucket.org [104.192.141.1] port 22.debug1: Connection established.debug1: identity file /c/Users/franck/.ssh/id_rsa type 0debug1: identity file /c/Users/franck/.ssh/id_rsa-cert type -1debug1: identity file /c/Users/franck/.ssh/id_dsa type -1debug1: identity file /c/Users/franck/.ssh/id_dsa-cert type -1debug1: identity file /c/Users/franck/.ssh/id_ecdsa type -1debug1: identity file /c/Users/franck/.ssh/id_ecdsa-cert type -1debug1: identity file /c/Users/franck/.ssh/id_ecdsa_sk type -1debug1: identity file /c/Users/franck/.ssh/id_ecdsa_sk-cert type -1debug1: identity file /c/Users/franck/.ssh/id_ed25519 type -1debug1: identity file /c/Users/franck/.ssh/id_ed25519-cert type -1debug1: identity file /c/Users/franck/.ssh/id_ed25519_sk type -1debug1: identity file /c/Users/franck/.ssh/id_ed25519_sk-cert type -1debug1: identity file /c/Users/franck/.ssh/id_xmss type -1debug1: identity file /c/Users/franck/.ssh/id_xmss-cert type -1debug1: Local version string SSH-2.0-OpenSSH_8.7debug1: Remote protocol version 2.0, remote software version conker_df6142773d 396556d9f365debug1: compat_banner: no match: conker_df6142773d 396556d9f365debug1: Authenticating to bitbucket.org:22 as 'git'debug1: load_hostkeys: fopen /c/Users/franck/.ssh/known_hosts2: No such file or directorydebug1: load_hostkeys: fopen /etc/ssh/ssh_known_hosts: No such file or directorydebug1: load_hostkeys: fopen /etc/ssh/ssh_known_hosts2: No such file or directorydebug1: SSH2_MSG_KEXINIT sentdebug1: SSH2_MSG_KEXINIT receiveddebug1: kex: algorithm: curve25519-sha256@libssh.orgdebug1: kex: host key algorithm: ssh-rsadebug1: kex: server-&gt;client cipher: chacha20-poly1305@openssh.com MAC: &lt;implicit&gt; compression: nonedebug1: kex: client-&gt;server cipher: chacha20-poly1305@openssh.com MAC: &lt;implicit&gt; compression: nonedebug1: expecting SSH2_MSG_KEX_ECDH_REPLYdebug1: SSH2_MSG_KEX_ECDH_REPLY receiveddebug1: Server host key: ssh-rsa SHA256:zzZyXwVuTsRqPoNmLkkJYKwbHaxvSc0ojez9YXaGp1Adebug1: load_hostkeys: fopen /c/Users/franck/.ssh/known_hosts2: No such file or directorydebug1: load_hostkeys: fopen /etc/ssh/ssh_known_hosts: No such file or directorydebug1: load_hostkeys: fopen /etc/ssh/ssh_known_hosts2: No such file or directorydebug1: Host 'bitbucket.org' is known and matches the RSA host key.debug1: Found key in /c/Users/franck/.ssh/known_hosts:4debug1: rekey out after 134217728 blocksdebug1: SSH2_MSG_NEWKEYS sentdebug1: expecting SSH2_MSG_NEWKEYSdebug1: SSH2_MSG_NEWKEYS receiveddebug1: rekey in after 134217728 blocksdebug1: Will attempt key: /c/Users/franck/.ssh/id_rsa RSA SHA256:WvJsAbCdEfGhIjKlMnOP0YBFy1XWm2/P6jCL7cBmQU4debug1: Will attempt key: /c/Users/franck/.ssh/id_dsadebug1: Will attempt key: /c/Users/franck/.ssh/id_ecdsadebug1: Will attempt key: /c/Users/franck/.ssh/id_ecdsa_skdebug1: Will attempt key: /c/Users/franck/.ssh/id_ed25519debug1: Will attempt key: /c/Users/franck/.ssh/id_ed25519_skdebug1: Will attempt key: /c/Users/franck/.ssh/id_xmssdebug1: SSH2_MSG_SERVICE_ACCEPT receiveddebug1: Authentications that can continue: publickeydebug1: Next authentication method: publickeydebug1: Offering public key: /c/Users/franck/.ssh/id_rsa RSA SHA256:WvJsAbCdEfGhIjKlMnOP0YBFy1XWm2/P6jCL7cBmQU4debug1: Authentications that can continue: publickeydebug1: Trying private key: /c/Users/franck/.ssh/id_dsadebug1: Trying private key: /c/Users/franck/.ssh/id_ecdsadebug1: Trying private key: /c/Users/franck/.ssh/id_ecdsa_skdebug1: Trying private key: /c/Users/franck/.ssh/id_ed25519debug1: Trying private key: /c/Users/franck/.ssh/id_ed25519_skdebug1: Trying private key: /c/Users/franck/.ssh/id_xmssdebug1: No more authentication methods to try.git@bitbucket.org: Permission denied (publickey).fatal: Could not read from remote repository.debug1: Offering public key: /c/Users/franck/.ssh/id_rsa RSA SHA256:WvJsAbCdEfGhIjKlMnOP0YBFy1XWm2/P6jCL7cBmQU4It is now obvious git is using the wrong key SHA256:WvJsAbCdEfGhIjKlMnOP0YBFy1XWm2/P6jCL7cBmQU4 instead of SHA256:XBzl9AbCdEfGhIjKlMnOPCm3VfP9Z9M9/iB8dG7dw24. You can actually see the key is sourced from the default file .ssh/id_rsa (which is what I defaulted to when setting up SSH for github) instead of the correct .ssh/bitbucket_id_rsa I used when setting up SSH with bitbucket. How do I fix this? I follow these instructions: Using Multiple SSH Keys for Multiple GitHub Accounts, which tell me to setup the SSH config file # GitLabHost gitlab.intranet.com   HostName gitlab.intranet.com   User git   IdentityFile ~/.ssh/id_rsa   # BitBucketHost bitbucket.org   HostName bitbucket.org, 123.456.789.1   User git   IdentityFile ~/.ssh/bitbucket_id_rsaThe Host bit must match the name you access your repo under, i.e. the bit after the @. Took me a while to figure that out until I found this post : How to configure multiple Git accounts in your computer which made it a bit clearer. Twist to the tale: turns out all this had already been very clearly documented by my colleagues, and I had actually RTFM myself, but somehow felt the need to reinvent the wheel anyway. Still, now I now more about investigating git issues‚Ä¶ ","categories": [],
        "tags": ["git","ssh"],
        "url": "https://franck-chester.github.io//2021/10/18/Setting-up-git-ssh-with-multiple-repos.html"
      },{
        "title": "Architecture descriptions in the cloud",
        "excerpt":"This week I have started documenting our target architecture. Now, what I was actually asked to do was ‚Äúdocument our target cloud infrastructure‚Äù, but I have been there before and believe a formal (ish) overall architecture description is required before diving into any specific problem area. I am going to use this post to describe how I use architecture descriptions and other architecture modelling techniques. I do not claim this is the only way or even the right way, but it works for me so might work for you as well. Architecture description What do I mean by an architecture description? I mean Systems and software engineering ‚Äî Architecture descriptionISO/IEC/IEEE 42010 which is an old fashion standard I was once required to adhere to for a government project. Now, in those waterfall days, my role as software architect was to design the solution upfront before delivering a formal, all encompassing description of it to the development teams. At the time it was easy, and a good idea, to use the viewpoints and perspectives listed in Rozanski and Woods ‚ÄúSoftware Systems Architecture‚Äù(R&amp;W) book. In particular the book (but sadly not the website) helped me examine my architecture through different angles and lenses, and provided a lot of checklists to validate both the solution and my description of it. Today, with lean and agile development processes adopted wholesale throughout the industry, I can be an agile architect, and never again create such a big design up front. I am however still a big fan of ‚Äòproper‚Äô architecture supported by appropriate descriptions, which often puts me at odd with the Agile (with a big A) crowd who misunderstand ‚ÄúWorking software over comprehensive documentation‚Äù as meaning ‚Äúno documentation‚Äù. I, personally, have found that the tools and techniques used to put an architecture descriptions are essential to support the conversations and eventual decisions that we take as a team, and to convey these decisions at various level of the organisation (what Gregor Hohpe calls ‚Äúriding the architect elevator to connect the penthouse with the engine room‚Äù). My architecture descriptions now pick and choose from a multitude of modelling techniques, whatever works to best communicate a particular aspect of the solution to my stakeholders. Format As much as I love a good diagram, I make a point to use both text and diagrams, the rule being that any important information appearing in the latter will be reflected in the former - that is I never rely solely on visual information. Vice-versa, I can be quite verbose so always try to reflect what I write in supporting diagrams. The added advantages of doing so are threefold:   the act of comparing text and diagram forces me to see both through the eye of my stakeholders, and identify incongruences.  iterating between text and diagram, adding or removing information from one to better align with the other generally improves the content, usually by simplifying it.  I get a ready made powerpoint slide - show the picture on screen and use the text as my spiel.I have tons of books on the Unified Modelling Language (UML) and have used it extensively in my career, and know for a fact that most people don‚Äôt care for it, don‚Äôt understand it and find it ugly. And to be fair, they are mostly right. When documenting a cloud architecture, it is preferable to use the prettier, and now ubiquitous cloud architecture format (square icons, boxes and lines) to describe the technical elements.Where I need to include more normative information, I do still use UML, but limit myself to component and sequence diagrams, which are relatively accessible. For interactions I sometime use extremely parred down activity diagrams but find that business stakeholders prefer the Business Process Modelling Notation (BPMN). Finally, for logical views, I give myself more artistic licence, and use whatever shapes and colours I need to describe any specific concept. What is essential however is consistency between diagrams across the whole architecture description. Granularity The architecture description provides 3 increasing levels of details, roughly aligned with C4 Modelling techniques, to support communication with a wide range of stakeholders, not all of which have an understanding of our technological or business domain. The high-level (macro) view will consider the elements of the solution that are our responsibility, and how these elements relate to the wider architecture of the enterprise. It very much aligns with the notion of Context in C4 modelling, and as such supports conversations with non technical stakeholders outside our team, as well as colleagues having only just joined us. The intermediate (meso) view will consider the major elements of our solution, and how they relate to each other, which aligns it with the C4 Container diagram, and support conversations with non technical stakeholders within our team or outside it but aligned to our business domain. Intermediate views are also a good basis to start technical conversations, or to reproduce ad-hoc on a physical or electronic whiteboard. The detailed (micro) view will consider architectural and design patterns used to implement specific aspects of our solution, very much like C4 Component diagram. Detailed views can be used to support decision making at team level (aka detailed design) as well as more formal process gates such as Technical Design Authority reviews or change control management. It is important however to note that the detailled design artefacts are not themselves part of the architecture description, but refer to the patterns and guidelines documented within it. Viewpoints Viewpoints are an ISO/IEC/IEEE 42010 - Systems and software engineering ‚Äî Architecture description archetype. There are and have been a multitude of so-called view models in our industry, but my favourite is the approach described by Rozanski and Woods (R&amp;W) by in their book ‚ÄúSoftware System Architecture‚Äù, which I have adapted with a set of viewpoint better aligned (imo) with agile and cloud architecture. Context viewpoint The context viewpoint was apparently a late addition by R&amp;W to the second edition of their book, but is one I believe is essential to provide a solid foundation to architectural discussions and decisions. They define it as:   The Context view of a system defines the relationships, dependencies, and interactions between the system and its environment‚Äîthe people, systems, and external entities with which it interacts. It defines what the system does and does not do; where the boundaries are between it and the outside world; and how the system interacts with other systems, organizations, and people across these boundaries. C4 Modelling defines it as:   this is your zoomed out view showing a big picture of the system landscape. The focus should be on people (actors, roles, personas, etc) and software systems rather than technologies, protocols and other low-level details. It‚Äôs the sort of diagram that you could show to non-technical people. Functional viewpoint R&amp;W define it as:   The view documents the system‚Äôs functional structure-including the key functional elements, their responsibilities, the interfaces they expose, and the interactions between them. Taken together, this demonstrates how the system will perform the functions required of it. whereas the 4+1 model calls it the ‚Äòlogical view‚Äô:   The logical architecture primarily supports the functional requirements‚Äîwhat the system should provide in terms of services to its users. The system is decomposed into a set of key abstractions, taken (mostly) from the problem domain, in the form of objects or object classes. They exploit the principles of abstraction, encapsulation, and inheritance. This decomposition is not only for the sake of functional analysis, but also serves to identify common mechanisms and design elements across the various parts of the system. A key word in the 4+1 definition is abstractions: the principal role of the functional viewpoints for is to name things. This is where we introduce the vocabulary that will be reused throughout the architecture description, its ubiquitous language (a term originating from Domain Driven Design), the rule of which is ‚Äúa single name for each thing, and only one thing identified by any given name‚Äù (my clunky wording). We must give ourselves a vocabulary precise enough to support architecture discussions and decisions without having to constantly explain what we mean. And this vocabulary must abstract lower level implementation details, to stop these discussions getting bogged down. When drawn, the functional view will make heavy use of non-descript boxes and arrows, ideal for electronic and physical whiteboarding sessions. Infrastructure viewpoint This is my preferred term over R&amp;W ‚Äòdeployment‚Äô and 4+1 ‚Äòphysical‚Äô viewpoints. This is where we start naming technologies and drawing cloud diagrams. What is important here is to maintain traceability between abstractions (things we described in our logical views) and their physical implementation.  This will stop us from suffering from the common disconnect between the fluffy abstraction and the nitty gritty of their implementation that has given both architecture and infrastructure such a bad name. From an architecture description point of view, we must also refrain from describing every nut and bolt of our infrastructure, as there is no value in doing so. What we describe here as the elements that personify our architecture decisions, with just enough detail to explain how they do so, and no more. Development viewpoint Most view models have a development view, but mean different things by it. 4+1 describes it as:   [‚Ä¶] related to the ease of development, software management, reuse or commonality, and to the constraints imposed by the toolset, or the programming language R&amp;W as    [‚Ä¶] include code structure and dependencies, build and configuration management of deliverables, system-wide design constraints, and system-wide standards to ensure technical integrity. It is the role of the Development view to address these aspects of the system development process. In our architecture description it will be used to bridge the gap between the what - as documented in the Logical and Infrastructure views - and the how, that is how we as a tribe go from wanting a thing to have that thing up and running in production. We will document where architecture decisions are constrained by our software development lifecycle (SDLC) and vice-versa. For example our choice of development language might limit our choice of infrastructure components or design patterns. The way we organise ourselves, our teams topologies, might impact how we assign responsibilities to specific software and/or infrastructure components. Our agile development processes and our adoption of DevOps and DevSecOps practices will also influence our architecture. Information viewpoint This viewpoint is again inspired from R&amp;W:   [‚Ä¶] high-level view of static information structure and dynamic information flow, with the objective of answering the big questions around ownership, latency, references, and so forth Here we adapt and extend it to cover description of our data model, how and what information flows through our systems and, how and where it is stored and accessed from and who by. This will support our Domain Driven Design, the definition or our interfaces, as well as threat modelling and compliance with the likes of General Data Protection Regulation (GDPR) and Payment Card Industry Data Security Standards (PCI DSS). Perspectives These are a concept introduced by R&amp;W and provide an elegant mechanism to address the non-functional attributes, also known as the quality attributes, or ‚Äò-ities‚Äô (security, availability, usability‚Ä¶). Rather than consider these concerns on their own, and risk being disconnected from the realities of our architecture, we acknowledge they are cross-cutting through the viewpoints described above. This also enables us to use checklists to reassure ourselves our architecture description doesn‚Äôt have any blind spots. These checklists can (and should) also be used outside the architecture description as part of or day to day design activities, such as 3 amigo sessions, or even work items‚Äô definition of ready. Unfortunately, R&amp;W Perspective catalogue does not map very well (imo) to a modern cloud based architecture. We will instead align our architecture description with the five pillars of well architected cloud solutions (e.g. AWS, Azure and GCP): Operational perspective We must ensure that our architecture supports our ability to develop, deploy, run and support the solution and its individual components. This perspective allows us to question the element described in any given viewpoint against this criteria. Interestingly, R&amp;W treated Operational attributes via a viewpoint of its own rather than a perspective, but with DevOps, that aspect in my mind is subsummed in the Development viewpoint. AWS Well-Architected Framework Operational Excellence Pillar whitepaper is an essential read to understand operational attributes of our architecture. Security perspective Here we consider whether the elements described by a given viewpoint impact:       security - how we prevent unauthorized access to organizational assets such as computers, networks, and data, and maintain the confidentiality, integrity and accessibility (so called CIA triad) sensitive and/or business critical information.         privacy - how we control access to Personally Identifiable Information  (PII) and other sensitive information and how it is used.         compliance - how we meet legal and regulatory requirements such as GDPR and PCI.   See also AWS Well-Architected Framework Security Pillar whitepaper. Reliability perspective This perspective validates that the elements described by a given viewpoint support our ability to perform correctly and consistently, and recover from failure inside or outside our control, at any scale. See also AWS Well-Architected Framework Reliability Pillar whitepaper. Performance perspective Here we consider how the elements described by a given viewpoint support our ability to perform consistently and efficiently regardless of the demand placed on our systems by their users. Cloud infrastructure offers elasticity, i.e. can scale in and out on demand, which must be taken into account when architecting our solution. See also AWS Well-Architected Framework Performance Efficiency Pillar whitepaper. Cost perspective Cloud infrastructure, and the ability to consume solutions ‚Äòas a Service‚Äô require us to have a clear understanding of the cost implications of our architecture decisions. The choice of components and patterns will greatly affect the overall cost of our solution, as well as the individual cost per-click/transaction. This must be considered as early as possible and become a first class concern in our decision process, weighed equally against sexier technical concerns. See also AWS Well-Architected Framework Cost Optimization Pillar whitepaper. Summary Rather than a big monolithic amorphous chunk of documentation, architecture descriptions can be sliced and diced over 3 axes, with each cell hopefully supporting discussion and decision making within and without our team.  I will hopefully revisit the subject in later posts, notably to document my checklists and how and where I use them. ","categories": [],
        "tags": ["architecture"],
        "url": "https://franck-chester.github.io//2021/10/22/Architecture-descriptions-in-the-cloud.html"
      },{
        "title": "Terraform CDK - part 1",
        "excerpt":"In this post I start building infrastructure components in my AWS sandbox, using the recently released Terraform Cloud Development Kit (CDK).I am going to keep it very basic, simply create an IAM role and policy, just to get myself going. What I am going to do however is dig into each little command and instructions I found in various tutorials, to make sure I understand the magic they hide from me. This builds up from my previous post: Scripting access to my AWS sandbox.Again, do note that I run a windows machine and use the powershell terminal. Terraform Terraform itself is a very popular, very much de-facto standard, open standard ‚ÄòInfrastructure as Code‚Äô (IaC) tool, created by Hashicorp for provisioning cloud infrastructures. It uses configuration files, written in Hashicorp Configuration Language to describe what we want our infrastructure to look like.When executed, Terraform will compare the actual infrastructure with that desired end state, and programmatically create or destroy resources to match.Where resources have dependencies on each other, it is smart enough to create (and later destroy) them in the right order. These files can be source controlled and code reviewed, and the execution of terraform entirely automated, thus ensuring consistency and reproducibility between deployments, even across multiple environments, eliminating human error and greatly reducing the time it takes to setup infrastructure. Terraform uses the concept of providers, , that implement a standard interface over the target infrastructure components API and can then be configured via HCL. I got quite adept at terraforming in my previous role, including creating my own custom provider in Go to workaround a then shortcoming in the Datadog official provider. HCL is great but quickly becomes a pain to work with when your target infrastructure is dynamic. As soon as you need to loop or assert, you find yourself hacking and/or writing hard to read and maintain HCL. It is also, a very ugly language to work with. Terraform CDK Announced in summer 2020, Terraform Cloud Development Kit is a programmatic layer used to generate HCL configurations. This brings a host of advantages.   You get to pick and leverage your development language of choice.  You can use said language to create abstraction over your infrastructure, so that instead of referring the an ‚ÄòEC2 instance‚Äô your code can refer to ‚ÄòNGNIX server‚Äô  You have (or at least eventually will get) more say over your IaC workflow, generate your HCL configuration from programmatic triggers, maybe pull data from external APIs to decide what to build or destroy, etc.It also brings a lot of pain, mainly because it adds a few more layers between your and the actual HCL files that act as the source of truth for your infrastructure. Throughout the course of writing this post, I had to fight issues with typescript, node.js, the terraform CDK, the terraform CDK command line interface (CLI) and the generated HCL itself. Great learning exercise, but could be a pain in production. To be fair, most of this pain is down to 1) my being new to typescript and node.js, and 2) the Terraform CDK still being in development, and admittedly not ready for production. In fact, as I spread my exploration over a few hours every friday evening, I was (un)lucky enough to hit a few releases and breaking changes. I have now learned to check the changelog before starting each experiment. Installing the Terraform CDK Download nodejs installer from https://nodejs.org/en/ or use chocolatey &gt; choco install nodejs-ltsDownload the yarn package manager from https://classic.yarnpkg.com, or use chocolatey &gt; choco install yarnInstall the CDK for Terraform globally: &gt; npm install -g cdktf-cliFinally1, install typescript itself, globallu: npm install -g typescriptCDK Terraform project initialisation We create a new empty folder (day03) and run the cdktf init command to initialise a brand new project: day03&gt; cdktf init --template=typescript --localNewer version of Terraform CDK is available [0.6.3] - Upgrade recommendedNote: By supplying '--local' option you have chosen local storage mode for storing the state of your stack.This means that your Terraform state file will be stored locally on disk in a file 'terraform.&lt;STACK NAME&gt;.tfstate' in the root of your project.? projectName: day03? projectDescription: Day 3 of my playing with the sandbox: let's terraform something...npm notice created a lockfile as package-lock.json. You should commit this file.+ constructs@10.0.0+ cdktf@0.6.2added 51 packages from 26 contributors and audited 51 packages in 29.859s       5 packages are looking for funding  run `npm fund` for details      found 0 vulnerabilitiesnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^2.3.2 (node_modules\\jest-haste-map\\node_modules\\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.3.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"win32\",\"arch\":\"x64\"})+ ts-jest@27.0.5+ jest@27.2.4+ @types/jest@27.0.2+ @types/node@16.10.2+ typescript@4.4.3added 332 packages from 296 contributors and audited 385 packages in 97.719s29 packages are looking for funding  run `npm fund` for details       found 0 vulnerabilities========================================================================================================      Your cdktf typescript project is ready!  cat help                Print this message  Compile:    npm run get           Import/update Terraform providers and modules (you should check-in this directory)    npm run compile       Compile typescript code to javascript (or \"npm run watch\")    npm run watch         Watch for changes and compile typescript in the background    npm run build         Compile typescript  Synthesize:    cdktf synth [stack]   Synthesize Terraform resources from stacks to cdktf.out/ (ready for 'terraform apply')  Diff:    cdktf diff [stack]    Perform a diff (terraform plan) for the given stack  Deploy:    cdktf deploy [stack]  Deploy the given stack  Destroy:    cdktf destroy [stack] Destroy the stack  Test:    npm run test        Runs unit tests (edit __tests__/main-test.ts to add your own tests)    npm run test:watch  Watches the tests and reruns them on change  Upgrades:    npm run upgrade        Upgrade cdktf modules to latest version    npm run upgrade:next   Upgrade cdktf modules to latest \"@next\" version (last commit) Use Prebuilt Providers:  You can add one or multiple of the prebuilt providers listed below:  npm install @cdktf/provider-aws  npm install @cdktf/provider-google  npm install @cdktf/provider-azurerm  npm install @cdktf/provider-docker  npm install @cdktf/provider-github  npm install @cdktf/provider-null  You can also build any module or provider locally. Learn more https://cdk.tf/modules-and-providers========================================================================================================Turns out I‚Äôm already out of date2 so need to upgrade to v0.6.03 with npm run upgrade, which execute the run-script command the upgrade script defined in the package.json file : npm i cdktf@latest cdktf-cli@latest . day03&gt; npm run upgrade&gt; day03@1.0.0 upgrade C:\\_workspaces\\tvg-sandbox\\day03&gt; npm i cdktf@latest cdktf-cli@latest&gt; core-js-pure@3.18.1 postinstall C:\\_workspaces\\tvg-sandbox\\day03\\node_modules\\core-js-pure&gt; node -e \"try{require('./postinstall')}catch(e){}\"&gt; @apollo/protobufjs@1.2.2 postinstall C:\\_workspaces\\tvg-sandbox\\day03\\node_modules\\@apollo\\protobufjs&gt; node scripts/postinstallnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.3.2 (node_modules\\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.3.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"win32\",\"arch\":\"x64\"})+ cdktf-cli@0.6.3+ cdktf@0.6.3added 418 packages from 308 contributors, updated 50 packages and audited 803 packages in 235.981s97 packages are looking for funding  run `npm fund` for detailsfound 0 vulnerabilitiesAs I am targetting my AWS sandbox, I need to install the AWS pre-built provider: day03&gt;  npm install @cdktf/provider-awsnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.3.2 (node_modules\\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.3.2: wanted {\"os\":\"darwin\",\"arch\":\"any\"} (current: {\"os\":\"win32\",\"arch\":\"x64\"})+ @cdktf/provider-aws@2.0.11added 1 package from 1 contributor and audited 805 packages in 45.086s97 packages are looking for funding  run `npm fund` for detailsfound 0 vulnerabilitiesNow, I got very confused at that point, and I am going to blame the docs and various examples for it. @cdktf/provider-aws is a ‚Äòpre-built provider‚Äô - that is, already in node.js format, installed in the standard node_modules folder, and imported from @cdktf/, e.g. import { AwsProvider} from '@cdktf/provider-aws'; Other types of providers, such as the Archive provider need to be generated locally first:   add the provider to the cdktf.json file - for example     \"terraformProviders\": [ \"hashicorp/archive@~&gt;2.2.0\" ]        run cdktf get. This will          pull the provider from the terraform registry,      generate the typescript constructs for that provider, that is the code that when executed will synthesise the proper terraform constructs for that provider.      These constructs get generated in the .gen folder and are imported from ./.gen/providers/, e.g. import { ArchiveProvider, DataArchiveFile } from \"./.gen/providers/archive\". A lot, if not most of the examples out there still use a local AWS provider, rather than the prebuilt one. This initially caused me to both install the pre-built modules, then build a local copy as well, and ignore the pre-built one. Create an execution role My aim is eventually to port the infrastructure elements of the API gateway tutorial to the Terraform CDK. The first step is to create an execution role. This AWS Identity and Access Management (IAM) role uses a custom policy to give your Lambda function permission to access the required AWS resources. Note that you must first create the policy and then create the execution role. Code the execution role I edit the generated main.ts file to import the AWS provider and IAM resources from the pre-built provider in stalled earlier: import { AwsProvider} from '@cdktf/provider-aws';import { IAM } from '@cdktf/provider-aws';I can now control click on the type and find the constructor definition: /*** Create a new {@link https://www.terraform.io/docs/providers/aws/r/iam_policy.html aws_iam_policy} Resource.** @param scope The scope in which to define this construct.* @param id The scoped construct ID.* @stability stable*/constructor(scope: Construct, id: string, config: IamPolicyConfig);which I use to guess the following code: main.ts: import { Construct } from 'constructs';import { App, TerraformStack} from 'cdktf';import { AwsProvider} from '@cdktf/provider-aws';import { IAM } from '@cdktf/provider-aws';class MyStack extends TerraformStack {  constructor(scope: Construct, id: string) {    super(scope, id)    new AwsProvider(this, 'aws', {      region: 'eu-west-1',      profile: \"franck-iac\"    })    const policy = {      \"Version\": \"2012-10-17\",      \"Statement\": [        {          \"Sid\": \"\",          \"Resource\": \"*\",          \"Action\": [            \"logs:CreateLogGroup\",            \"logs:CreateLogStream\",            \"logs:PutLogEvents\"          ],          \"Effect\": \"Allow\"        }      ]    };    new IAM.IamPolicy(this, 'lambda_apigateway_policy', {      name:'lambda_apigateway_policy',      description: 'Access rights for my API Gateway lambda, as per https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-tutorial.html#services-apigateway-tutorial-role',      policy:JSON.stringify(policy)    });  }}const app = new App()new MyStack(app, \"day03\");app.synth();Generate the HCL for the execution role I can execute the above main.ts file with the (cdktf synth) command to generate (aka synthesise) the equivalent terraform configuration files: cdktf.out\\stacks\\day03\\cdk.tf.json: {  \"//\": {    \"metadata\": {      \"version\": \"0.7.0\",      \"stackName\": \"day03\",      \"backend\": \"local\"    }  },  \"terraform\": {    \"required_providers\": {      \"aws\": {        \"version\": \"~&gt; 3.0\",        \"source\": \"aws\"      }    }  },  \"provider\": {    \"aws\": [      {        \"profile\": \"franck-iac\",        \"region\": \"eu-west-1\"      }    ]  },  \"resource\": {    \"aws_iam_policy\": {      \"lambda_apigateway_policy\": {        \"description\": \"Access rights for my API Gateway lambda, as per https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-tutorial.html#services-apigateway-tutorial-role\",        \"name\": \"lambda_apigateway_policy\",        \"policy\": \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Sid\\\":\\\"\\\",\\\"Resource\\\":\\\"*\\\",\\\"Action\\\":[\\\"logs:CreateLogGroup\\\",\\\"logs:CreateLogStream\\\",\\\"logs:PutLogEvents\\\"],\\\"Effect\\\":\\\"Allow\\\"}]}\",        \"//\": {          \"metadata\": {            \"path\": \"day03/lambda_apigateway_policy\",            \"uniqueId\": \"lambda_apigateway_policy\"          }        }      }    }  }}Plan and apply the generated configuration The generated terraform configuration results in the following terraform plan (cdktf plan): &gt; cdktf plan Stack: day03Resources + AWS_IAM_POLICY       lambda-apigateway-p aws_iam_policy.lambda-apigateway-policyDiff: 1 to create, 0 to update, 0 to delete.We are starting from a blank sandbox and therefore we should only need to create one new resource, for the policy. Let‚Äôs go wild and apply this to the sandbox &gt; cdktf apply‚†á Deploying Stack: day03Resources ‚†º AWS_IAM_POLICY       lambda-apigateway-p aws_iam_policy.lambda-apigateway-policySummary: 0 created, 0 updated, 0 destroyed.[2021-10-29T13:41:41.946] [ERROR] default - ‚ï∑‚îÇ Error: error creating IAM policy lambda-apigateway-policy: AccessDenied: User: arn:aws:iam::012345678910:user/franck-iac is not authorized to perform: iam:CreatePolicy on resource: policy lambda-apigateway-policyThis AccessDenied error makes sense, the IAM policy I attached to my IaC user is a dummy one (see first post in the series), I haven‚Äôt actually tailored it to this project. Revisit our Terraform‚Äôs AWS profile access rights Now, sticking with a least privilege approach with AWS can be a struggle.I usually do as this blog suggests and use the console. Here, given the error message iam:CreatePolicy, let‚Äôs navigate to the IAM service, and look under either ‚Äòwrite‚Äô (usually associated with access right for the creation of stuff) and ‚Äòpermission management‚Äô. Bingo, it‚Äôs under the latter:  Now, add a resource. The console is quite emphatic that a specific resource should be specified, and quite right too: * is the root of all evil.Here, we do not have a specific resource, but we can specify will will only create policy in the current account. Let‚Äôs recreate our iac-policy.json file as: {    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Sid\": \"IaC01\",            \"Effect\": \"Allow\",            \"Action\": \"iam:CreatePolicy\",            \"Resource\": \"arn:aws:iam::012345678910:policy/*\"        }    ]}This gives our IaC user the right to create a new policy in our account, and our account only.I can use my handy update-policy.ps1 script) to easily update my IaC user‚Äôs right with this policy. Let‚Äôs try again: &gt; cdktf apply...AccessDenied: User: arn:aws:iam::012345678910:user/franck-iac is not authorized to perform: iam:GetPolicy on resource: policy arn:aws:iam::012345678910:policy/lambda-apigateway-policyGetting there. For the lack of comprehensive documentation, I am going to go a few rounds like this, allowing additional IAM action in IaC user policy one by one.The advantage is that I can check the IAM actions individually to understand what is going on under the hood. The disadvantage is that the process is slow and painful. I have yet to find a smarter way to do this, and I am not on my own - as per this terraform issue, and stackoverflow.I will, one day, experiment with iamlive, which would theoretically allow me to execute my terraform configuration with a super -user, log the corresponding access rights and then add these to my IaC user policy. Anyway, my iac-policy.json ends up looking like this : {    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Sid\": \"IaC01\",            \"Effect\": \"Allow\",            \"Action\": [                \"iam:CreatePolicy\",                \"iam:GetPolicy\",                \"iam:GetPolicyVersion\",                \"iam:ListPolicyVersions\",                \"iam:DeletePolicy\"            ],            \"Resource\": \"arn:aws:iam::012345678910:policy/*\"        }    ]}Success &gt; cdktf applyDeploying Stack: day03Resources ‚úî AWS_IAM_POLICY       lambda_apigateway_policy aws_iam_policy.lambda_apigateway_policySummary: 1 created, 0 updated, 0 destroyed.Hurrah!: Let‚Äôs peek at the IAM console‚Ä¶ Result: we have terraformed a policy via the CDK! :smile:  Create the role Add this to main.ts: const lambda_assume_role_policy = {      \"Version\": \"2012-10-17\",      \"Statement\": [        {          \"Effect\": \"Allow\",          \"Principal\": {            \"Service\": \"lambda.amazonaws.com\"          },          \"Action\": \"sts:AssumeRole\"        }      ]    };new IAM.IamRole(this, 'lambda-apigateway-role', {  name: 'lambda-apigateway-role',  assumeRolePolicy: JSON.stringify(lambda_assume_role_policy)});Error: error creating IAM Role (lambda-apigateway-role): AccessDenied: User: arn:aws:iam::012345678910:user/franck-iac is not authorized to perform: iam:CreateRole on resource: arn:aws:iam::012345678910:role/lambda-apigateway-roleHere we go again‚Ä¶ After a few tries and errors, these are the access rights we need to add to the IaC role.Note that I add them to iac-policy.json as a separate policy statement, to try and keep track of what right is used for what. {    \"Sid\": \"IaC02\",    \"Effect\": \"Allow\",    \"Action\": [        \"iam:CreateRole\",        \"iam:GetRole\",        \"iam:ListRolePolicies\",        \"iam:ListAttachedRolePolicies\",        \"iam:ListInstanceProfilesForRole\",        \"iam:DeleteRole\"    ],    \"Resource\": \"arn:aws:iam::012345678910:role/*\"}&gt; cdktf applyDeploying Stack: day03Resources ‚úî AWS_IAM_ROLE         lambda-apigateway-role aws_iam_role.lambda-apigateway-roleSummary: 1 created, 0 updated, 0 destroyed.Our code didn‚Äôt change the policy, therefore it hasn‚Äôt been updated or destroyed.The role itself is new, therefore has been created as a new resource, which can can see in the IAM console:  Cleanup When experimenting in the cloud is it good practice to clean up as soon as we‚Äôre done, and save ourselves money. Doesn‚Äôt really matter here, roles and policies don‚Äôt incur costs, but a good habit to get into. &gt; cdktf destroyDestroying Stack: day03Resources ‚úî AWS_IAM_POLICY       lambda_apigateway_policy aws_iam_policy.lambda_apigateway_policy ‚úî AWS_IAM_ROLE         lambda-apigateway-role aws_iam_role.lambda-apigateway-roleSummary: 2 destroyed.&gt; .\\teardown.ps1Delete Access Key for IaC user 'franck-iac' ...Detach policy  to user 'franck-iac'...Delete IaC user 'franck-iac' ...Delete access policy 'franck-iac' ...Sandbox IaC user teardown complete ...Conclusion This is all very basic but I now have a good understanding of the Terraform CDK and its foibles. I have also tested my developer workflow, and exercised my IaC scripts. I will continue3 this experiment in a later post, deploy a lambda function, configure the API Gateway to invoke it, etc.             This step is undocumented elsewhere, and maybe not actually required.However, what was meant to be a quick experiment with the CDK ended up spread over 5 attempts: Things went very wrong, I kept hitting MODULE_NOT_FOUND errors when running cdktf synth. The code and all paths were fine, I and vscode could see all the modules, but cdktf synth kept failing.On what was actually Day 07, I decided to ignored the cdktf commands and compile the typescript code directly (tsc --build --clean, tsc --build --verbose), which required me to install typescript (npm install -g typescript), which then somehow got rid of my MODULE_NOT_FOUND errors.I am therefore going to assume that installing typescript separately is a pre-requisite to using the terraform CDK. I won‚Äôt know for sure until retry it all on a clean machine, maybe spin a container for it.¬†&#8617;               A few weeks elapsed between my installing the tooling and actually trying to use it¬†&#8617;               and TIDY UP, as I have now spotted inconsistencies in my naming terraform resources (should use underscore rather than hyphens).¬†&#8617;       ","categories": [],
        "tags": ["iac","terraform","cdktf"],
        "url": "https://franck-chester.github.io//2021/10/30/Terraform-CDK-part-1.html"
      },{
        "title": "Terraform CDK - part 2",
        "excerpt":"It‚Äôs been 2 week since the previous post in this series and I am really starting to enjoy the Terraform CDK. Now that I am more comfortable with it, and with Typescript, I have started organising my code in much more (imo) expressive blocks, which I will later be able to move to a reusable nodejs module, should I wish to. That said, this exercise is a painful (but useful) reminder of the chasm between the simplistic tutorials available on the web, and reality.If you, as I do, insist on not using the AWS console, and adhere to best practices such as least privilege and strict separation of infrastructure and code, there are quite a few hoops and loops needed to be jumped through. Anyway, where were we? Translating this tutorial - Using Lambda with API Gateway - into Terraform CDK Typescript code. Create the function Separations of infrastructure and code concerns I want to make sure I keep the code separate from the infrastructure, both in separate code repositories.I should deploy the infrastructure first, then the actual code that powers my lambda function. I still need to research whether this is actually best practice, but my gut feeling is that using terraform to deploy the code for each new release is the wrong way to go.My plan is therefore to deploy the lambda infrastructure with a skeleton implementation, and once that is in place, use the update-function-code CLI command to deploy each new version of the actual implementation. Also, my first attempt at terraforming the lambda function for this tutorial was based on using the archive provider to zip the source code before using the zip file as a parameter of the lambda function resource, very much like this tutorial and every other tutorial out there, including the API Gateway one I am trying to reproduce. Now, this was already a bad idea in standard terraform, as it intermingles infrastructure and code deployments, and seems somehow worse in the CDK, probably because it is hard to stop myself from using the CDK code, which is meant to generate the HCL files which then deploy the infrastructure, to zip and deploy the code itself, which is really a separate concern. I had a few goes at this in various formats but at the end of the day, it smells. So, I am going to create a S3 bucket for code deployment, as part of my sandbox setup scripts, refer to it within my CDK code and later, when calling update-function-code. Use iamlive to create IaC user IAM policy I know from experience that creating a S3 bucket requires access rights to quite a few Actions. Rather than carry on with the painful one-at-a-time approach used in part 1, I am going to get smarter and use the iamlive utility. I first clone the repository, build and install the utility.I then add this new powershell script - iamlive.ps1 - to my setup folder: param([switch]$stop)if($stop.isPresent){    write-host \"Stopping iamlive...\"    Stop-Process -Name \"iamlive\"    $env:AWS_CSM_ENABLED=$false    write-host \"Stopping iamlive: done\"}else{    $env:AWS_CSM_ENABLED=$true    $env:AWS_CSM_PORT=31000    $env:AWS_CSM_HOST=127.0.0.1    write-host \"Starting iamlive...\"    Start-Process -FilePath \"$env:GOPATH/bin/iamlive.exe\"}This will start iamlive in a separate terminal, and proxy all AWS SDK calls (as used by terraform) through the iamlive utility, which will map them to the corresponding permissions. I also modify my main.ts file to source the AWS profile from a awsprofile environmental variable. I would have preferred a command line parameter, but these are currently not easy to pass down to the app in all cdktf commands. constructor(scope: Construct, id: string, profile: string) {    super(scope, id)    new AwsProvider(this, 'aws', {      region: 'eu-west-1',      profile: profile    })...const profile = ('awsprofile' in process.env) ? `${process.env.awsprofile}` : 'franck-iac';console.log(`Using AWS profile ${profile}`)const app = new App()new MyStack(app, \"day04\", profile);app.synth();I then execute cdktf apply with my admin profile, and get a nice policy in the terminal running iamlive: {    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": [                \"ec2:DescribeAccountAttributes\",                \"s3:ListBucket\",                \"iam:GetPolicy\",                \"iam:GetRole\",                \"iam:GetPolicyVersion\",                \"iam:ListRolePolicies\",                \"iam:ListAttachedRolePolicies\",                \"s3:GetBucketAcl\",                \"s3:GetBucketCORS\",                \"s3:GetBucketWebsite\",                \"s3:GetBucketVersioning\",                \"s3:GetAccelerateConfiguration\",                \"s3:GetBucketRequestPayment\",                \"s3:GetBucketLogging\",                \"s3:GetLifecycleConfiguration\",                \"s3:GetReplicationConfiguration\",                \"s3:GetEncryptionConfiguration\",                \"s3:GetBucketObjectLockConfiguration\",                \"s3:GetBucketTagging\",                \"s3:DeleteBucket\",                \"s3:CreateBucket\",                \"s3:PutBucketTagging\",                \"s3:PutBucketVersioning\",                \"iam:ListPolicyVersions\",                \"iam:ListInstanceProfilesForRole\",                \"iam:DeletePolicy\",                \"iam:DeleteRole\"            ],            \"Resource\": \"*\"        }    ]}I follow this with cdktf destroy, again with my admin profile, which updates the policy displayed in the terminal with the additional actions required to delete the infrastructure. I can now incorporate the missing actions in the IaC user policy. I‚Äôve also installed the sort json array vscode extension, to keep my iac-policy.json file tidy, and easily compare the output from iamlive to it, and identify what needs adding. Although this works remarkably well, it is far from perfect and can still miss some actions, probably because the iamlive map is incomplete or out of date. For example, it output the v1 permissions for the APIGateway, and completely missed the s3:DeleteObject and s3:DeleteObjectVersion, causing me quite a bit of faffing to identify the correct policy using the tried and tested combination of IAM console and documentation (Actions, resources, and condition keys for AWS services). The policy also needs the actions associated with updates, otherwise small updates to the CDK code will fail without destroying the stack entirely first before reapplying it. Finally, I will need to revisit this again later as this is still not a least privilege policy, as it still enables my IaC user to delete resources it hasn‚Äôt created.My next move, in a future experiment, will be using tags. Creating a S3 bucket for lambda code deployment Not much to say, we simply use the S3 Bucket resource, making sure to set forceDestroy: true to ensure all objects (including any locked objects) are from the bucket before destroying it 1 - otherwise we wouldn‚Äôt be able to tear down this infrastructure. import { S3 } from '@cdktf/provider-aws';...const sourceBucket = new S3.S3Bucket(this, 'lambda_source_bucket', {    bucket: 'franck-iac-lambda-source-bucket',    acl: 'private',    tags: tags,    versioning: {      enabled: false    },    forceDestroy: true  });Upload placeholder lambda source code to S3 Bucket We cannot create a lambda function without code behind it, but we can point the lambda to a barebone implementation which we‚Äôll later override with the actual code 2. exports.handler = function(event, context, callback) {    console.log('Received event:', JSON.stringify(event, null, 2));    const responseBody = {        \"warning\":\"Placeholder code - function not yet implemented\",        \"event\":event    }    var response = {        \"statusCode\": 200,        \"headers\": {            \"Content-Type\": \"application/json\"        },        \"body\": JSON.stringify(responseBody),        \"isBase64Encoded\": false    };    callback(null, response);};We do this with a combination of Terraform assets and S3 Bucket object, as per this example. const sourceAsset = new TerraformAsset(this, \"lambda_asset\", {    path: path.resolve(__dirname, \"src\"),    type: AssetType.ARCHIVE, });const sourceBucketObject = new S3.S3BucketObject(this, \"lambda_archive\", {    bucket: sourceBucket.bucket!,   // exclamation mark is non-null-assertion-operator    key: sourceAsset.fileName,    source: sourceAsset.path,});NB: when using one resource‚Äôs attribute to set another‚Äôs, we often hit this error Type 'string | undefined' is not assignable to type 'string'.To work around this, we simply use the typescript non-null assertion operator (!). Creating a Lambda function sourced from a S3 Bucket First import the LambdaFunction type from the AWS provider.Here we are hitting the recently introduced namespaces. For IAM and S3, these looked fine, but for lambda functions, we would end up referring to LambdaFunction.LambdaFunction in our code, which would be pants. We therefore alias the type import { LambdaFunction as LambdaFunctionNS } from '@cdktf/provider-aws';const { LambdaFunction } = LambdaFunctionNS.LambdaFunction;...const lambda = new LambdaFunction(this, 'lambda', {    functionName: 'LambdaFunctionOverHttps',    description: 'as per https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-tutorial.html',    s3Bucket: sourceBucket.bucket!,    s3Key: sourceBucketObject.key,    role: LambdaApiGatewayRole.arn,    runtime: 'nodejs12.x',    handler: 'index.handler',    tags: tags});We point the lambda at the S3 bucket and object created earlier, from where it will load our skeleton implementation. Deploy to the sandbox Deploying Stack: day04Resources ‚úî AWS_IAM_POLICY       lambda_apigateway_p aws_iam_policy.lambda_apigateway_policy                        olicy ‚úî AWS_IAM_ROLE         lambda-apigateway-r aws_iam_role.lambda_apigateway_role                        ole                        ‚úî AWS_LAMBDA_FUNCTION  lambda              aws_lambda_function.lambda ‚úî AWS_S3_BUCKET        lambda_source_bucke aws_s3_bucket.lambda_source_bucket                        t ‚úî AWS_S3_BUCKET_OBJECT lambda_archive      aws_s3_bucket_object.lambda_archiveSummary: 5 created, 0 updated, 0 destroyed.Success. Test the lambda The tutorial and a lot of examples out there are out of date, CLI v2 defaults to base 64 input. You can either add a ‚Äìcli-binary-format raw-in-base64-out argument to the command: aws lambda invoke --profile admin-sandbox --function-name LambdaFunctionOverHttps --payload file://tests/helloworld.json --cli-binary-format raw-in-base64-out tests/output.txt OR specify the file with fileb://:  aws lambda invoke --profile admin-sandbox --function-name LambdaFunctionOverHttps --payload fileb://tests/helloworld.json  tests/output.txtEither way, with tests/helloworld.json set to: {    \"Hello\": \"world\"}Both call result in a success response : {    \"StatusCode\": 200,    \"ExecutedVersion\": \"$LATEST\"}with tests/output.txt: {    \"statusCode\": 200,    \"headers\": {        \"Content-Type\": \"application/json\"    },    \"body\": \"{\\\"warning\\\":\\\"Placeholder code - function not yet implemented\\\",\\\"event\\\":{\\\"Hello\\\":\\\"world\\\"}}\",    \"isBase64Encoded\": false}which matches what we expect - success. Create a REST API using API Gateway We need to mix and match and translate these 3 terraform HCL examples into their CDK equivalent   api_gateway_deployment  api_gateway_integration  api_gateway_accountThis will:   create a REST API  define a resource that can be manipulated via it  define an integration to our lambda function to handle operations against that resource  deploy and configure that REST API as ‚Äòstage‚Äô v1  configure our API Gateway account so that it can log to cloudwatchWorth noting as well that one thing I forgot to do in my previous post was to attach the policy to the role.Most examples use inline policies, but I prefer to use managed ones for consistency. Finally, we use a terraform output to display the deployed API URL on the console const restApi = new APIGateway.ApiGatewayRestApi(this, 'api_gateway', {    name: 'DynamoDBOperations',    description: 'as per https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-tutorial.html',    tags: tags,    endpointConfiguration: {        types: ['REGIONAL']    }});const apiGatewayResource = new APIGateway.ApiGatewayResource(this, 'api_gateway_resource', {    parentId: restApi.rootResourceId,    pathPart: \"dynamodbmanager\",    restApiId: restApi.id});const apiGatewayMethod = new APIGateway.ApiGatewayMethod(this, 'api_gateway_method_post', {    authorization: \"NONE\",    httpMethod: \"POST\",    resourceId: apiGatewayResource.id,    restApiId: restApi.id});const apiGatewayMethodIntegration = new APIGateway.ApiGatewayIntegration(this, 'api_gateway_integration', {    type: 'AWS_PROXY',    httpMethod: apiGatewayMethod.httpMethod, // the method to use when calling the API Gateway endpoint    integrationHttpMethod: 'POST',              //  the method used by API Gateway to call the backend (it should always be POST for Lambda)    resourceId: apiGatewayResource.id,    restApiId: restApi.id,    uri: lambda.invokeArn,    credentials: credentials.arn});const apiGatewayDeployment= new APIGateway.ApiGatewayDeployment(this, 'api_gateway_deployment', {    restApiId: restApi.id,    triggers: {        redeployment: Fn.sha1(Fn.jsonencode([apiGatewayResource.id, apiGatewayMethod.id, apiGatewayMethodIntegration.id]))    },    lifecycle: {        createBeforeDestroy: true    }});const apiGatewayDeploymentStage = new APIGateway.ApiGatewayStage(stack, 'api_gateway_stage', {    deploymentId: apiGatewayDeployment.id,    restApiId: apiGatewayDeployment.restApiId,    stageName: 'v1',    tags: tags});new APIGateway.ApiGatewayMethodSettings(stack, 'api-gateway-method-settings', {    restApiId: apiGatewayDeployment.restApiId,    stageName: apiGatewayDeploymentStage.stageName,    methodPath: \"*/*\",    settings: {        metricsEnabled: true,        dataTraceEnabled: true,        loggingLevel: 'INFO',        throttlingRateLimit: 100,        throttlingBurstLimit: 50    }});const apiGatewayPolicy = new IAM.IamPolicy(stack, 'api_gateway_policy', {    name: 'api_gateway_policy',    description: 'Access rights for my API Gateway - mainly read and write cloudwatch logs',    policy: JSON.stringify({      \"Version\": \"2012-10-17\",      \"Statement\": [        {          \"Effect\": \"Allow\",          \"Action\": [            \"logs:CreateLogGroup\",            \"logs:CreateLogStream\",            \"logs:DescribeLogGroups\",            \"logs:DescribeLogStreams\",            \"logs:PutLogEvents\",            \"logs:GetLogEvents\",            \"logs:FilterLogEvents\"          ],          \"Resource\": \"*\"        }      ]    }),    tags: tags  });const apiGatewayRole = new IAM.IamRole(stack, 'apigateway_role', {    name: 'apigateway_role',    description: 'IAM role for the API Gateway',    assumeRolePolicy: JSON.stringify({        \"Version\": \"2012-10-17\",        \"Statement\": [        {            \"Effect\": \"Allow\",            \"Principal\": {            \"Service\": \"apigateway.amazonaws.com\"            },            \"Action\": \"sts:AssumeRole\"        }        ]    })});new IAM.IamRolePolicyAttachment(stack, 'apigateway_role_policy_attachment', {    role: apiGatewayRole.name!,    policyArn: apiGatewayPolicy.arn});return new APIGateway.ApiGatewayAccount(stack, 'api_gateway_account', {    cloudwatchRoleArn: apiGatewayRole.arn});new TerraformOutput(this, \"invoke-url\", {    description: 'Invoke URL for the API',    value: apiGatewayDeploymentStage.invokeUrl});Deploy to the sandbox Deploying Stack: day04Resources ‚úî AWS_API_GATEWAY_ACCO api_gateway_account aws_api_gateway_account.api_gateway_acc   UNT                                      ount ‚úî AWS_API_GATEWAY_DEPL api_gateway_deploym aws_api_gateway_deployment.api_gateway_   OYMENT               ent                 deployment ‚úî AWS_API_GATEWAY_INTE api_gateway_method_ aws_api_gateway_integration.api_gateway   GRATION              post_integration_dy _method_post_integration_dynamodbmanage                        namodbmanager       r ‚úî AWS_API_GATEWAY_METH api_gateway_method_ aws_api_gateway_method.api_gateway_meth   OD                   post_dynamodbmanage od_post_dynamodbmanager                        r ‚úî AWS_API_GATEWAY_METH api-gateway-method- aws_api_gateway_method_settings.api-gat   OD_SETTINGS          settings            eway-method-settings ‚úî AWS_API_GATEWAY_RESO api_gateway_resourc aws_api_gateway_resource.api_gateway_re   URCE                 e_dynamodbmanager   source_dynamodbmanager ‚úî AWS_API_GATEWAY_REST api_gateway_rest_ap aws_api_gateway_rest_api.api_gateway_re   _API                 i_dynamodbmanager   st_api_dynamodbmanager ‚úî AWS_API_GATEWAY_STAG api_gateway_stage   aws_api_gateway_stage.api_gateway_stage   E ‚úî AWS_IAM_POLICY       api_gateway_policy  aws_iam_policy.api_gateway_policy ‚úî AWS_IAM_POLICY       lambda_apigateway_p aws_iam_policy.lambda_apigateway_policy                        olicy ‚úî AWS_IAM_POLICY       rest_api_policy_TfT aws_iam_policy.rest_api_policy_TfTokenT                        okenTOKEN9          OKEN9 ‚úî AWS_IAM_ROLE         apigateway_role     aws_iam_role.apigateway_role ‚úî AWS_IAM_ROLE         lambda_apigateway_r aws_iam_role.lambda_apigateway_role                        ole ‚úî AWS_IAM_ROLE         rest_api_role_TfTok aws_iam_role.rest_api_role_TfTokenTOKEN                        enTOKEN11           11 ‚úî AWS_IAM_ROLE_POLICY_ apigateway_role_pol aws_iam_role_policy_attachment.apigatew   ATTACHMENT           icy_attachment      ay_role_policy_attachment ‚úî AWS_IAM_ROLE_POLICY_ lambda_apigateway_r aws_iam_role_policy_attachment.lambda_a   ATTACHMENT           ole_policy_attachme pigateway_role_policy_attachment                        nt ‚úî AWS_IAM_ROLE_POLICY_ rest_api_role_polic aws_iam_role_policy_attachment.rest_api   ATTACHMENT           y_attachment_TfToke _role_policy_attachment_TfTokenTOKEN12                        nTOKEN12 ‚úî AWS_LAMBDA_FUNCTION  lambda              aws_lambda_function.lambda ‚úî AWS_S3_BUCKET        lambda_source_bucke aws_s3_bucket.lambda_source_bucket                        t ‚úî AWS_S3_BUCKET_OBJECT lambda_archive      aws_s3_bucket_object.lambda_archiveSummary: 20 created, 0 updated, 0 destroyed.Output: invoke-url = https://ruwqixs5g5.execute-api.eu-west-1.amazonaws.com/v1Looking good! 20 terraform resources though, for a simple API! Invoke our newly deployed API I am a big fan of the vscode REST client extension - much easier than the bloat of postman POST https://ruwqixs5g5.execute-api.eu-west-1.amazonaws.com/v1/dynamodbmanagerContent-Type: application/json{     \"hello\": \"world\"}results in HTTP/1.1 200 OKDate: Wed, 10 Nov 2021 22:02:33 GMTContent-Type: application/jsonContent-Length: 1738Connection: closex-amzn-RequestId: 9de3e048-306f-4ada-8e8b-cd7dd2d71d2ex-amz-apigw-id: Im8rAFZIjoEFuOw=X-Amzn-Trace-Id: Root=1-618c4179-18f4fd98090a20d23b2638d3;Sampled=0{  \"warning\": \"Placeholder code - function not yet implemented\",  \"event\": {    \"resource\": \"/dynamodbmanager\",    \"path\": \"/dynamodbmanager\",    \"httpMethod\": \"POST\",    \"headers\": {      \"accept-encoding\": \"gzip, deflate\",      \"content-type\": \"application/json\",      \"Host\": \"ruwqixs5g5.execute-api.eu-west-1.amazonaws.com\",      \"User-Agent\": \"vscode-restclient\",      \"X-Amzn-Trace-Id\": \"Root=1-618c4179-18f4fd98090a20d23b2638d3\",      \"X-Forwarded-For\": \"98.76.54.32\",      \"X-Forwarded-Port\": \"443\",      \"X-Forwarded-Proto\": \"https\"    },    \"multiValueHeaders\": {      \"accept-encoding\": [        \"gzip, deflate\"      ],      \"content-type\": [        \"application/json\"      ],      \"Host\": [        \"ruwqixs5g5.execute-api.eu-west-1.amazonaws.com\"      ],      \"User-Agent\": [        \"vscode-restclient\"      ],      \"X-Amzn-Trace-Id\": [        \"Root=1-618c4179-18f4fd98090a20d23b2638d3\"      ],      \"X-Forwarded-For\": [        \"98.76.54.32\"      ],      \"X-Forwarded-Port\": [        \"443\"      ],      \"X-Forwarded-Proto\": [        \"https\"      ]    },    \"queryStringParameters\": null,    \"multiValueQueryStringParameters\": null,    \"pathParameters\": null,    \"stageVariables\": null,    \"requestContext\": {      \"resourceId\": \"hagmic\",      \"resourcePath\": \"/dynamodbmanager\",      \"httpMethod\": \"POST\",      \"extendedRequestId\": \"Im8rAFZIjoEFuOw=\",      \"requestTime\": \"10/Nov/2021:22:02:33 +0000\",      \"path\": \"/v1/dynamodbmanager\",      \"accountId\": \"012345678910\",      \"protocol\": \"HTTP/1.1\",      \"stage\": \"v1\",      \"domainPrefix\": \"ruwqixs5g5\",      \"requestTimeEpoch\": 1636581753401,      \"requestId\": \"9de3e048-306f-4ada-8e8b-cd7dd2d71d2e\",      \"identity\": {        \"cognitoIdentityPoolId\": null,        \"accountId\": null,        \"cognitoIdentityId\": null,        \"caller\": null,        \"sourceIp\": \"12.34.56.78\",        \"principalOrgId\": null,        \"accessKey\": null,        \"cognitoAuthenticationType\": null,        \"cognitoAuthenticationProvider\": null,        \"userArn\": null,        \"userAgent\": \"vscode-restclient\",        \"user\": null      },      \"domainName\": \"ruwqixs5g5.execute-api.eu-west-1.amazonaws.com\",      \"apiId\": \"ruwqixs5g5\"    },    \"body\": \"{\\r\\n     \\\"hello\\\": \\\"world\\\"\\r\\n}\",    \"isBase64Encoded\": false  }}Again, result! Refactor the code So far, we have not really leveraged the move from HCL to a strongly typed language (typescript). Let‚Äôs do that now - group related resources in their own functions, and start using self-descriptive names for functions and variables to make it all easier to understand. And, while we are at it, mostly for the fun of it, because strictly speaking we have no need for it yet, let‚Äôs add some logic to enable us to define multiple methods on an resource, not just POST, but GET, PUT and DELETE as well, to cover all aspects of CRUD For example: function defineRestApiResourceDeployment(    stack: TerraformStack,     resourceName: string,     lambdas: {         create?: LambdaFunction,         read?: LambdaFunction,         update?: LambdaFunction,         delete?: LambdaFunction     },     credentials: IAM.IamRole) {    const restApi = new APIGateway.ApiGatewayRestApi(stack, `api_gateway_rest_api_${resourceName}`, {        name: resourceName,        description: 'as per https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-tutorial.html',        tags: tags,        endpointConfiguration: {            types: ['REGIONAL']        }    });    const apiGatewayResource = new APIGateway.ApiGatewayResource(stack, `api_gateway_resource_${resourceName}`, {        parentId: restApi.rootResourceId,        pathPart: resourceName,        restApiId: restApi.id    });    const redeploymentTriggerElements = [apiGatewayResource.id];    const defineMethodForResource = (method: string, lambda: LambdaFunction) =&gt; {    const apiGatewayMethod = new APIGateway.ApiGatewayMethod(stack, `api_gateway_method_${method.toLowerCase()}_${resourceName}`, {        authorization: \"NONE\",        httpMethod: method.toUpperCase(),        resourceId: apiGatewayResource.id,        restApiId: restApi.id,    });    const apiGatewayMethodIntegration = new APIGateway.ApiGatewayIntegration(stack, `api_gateway_method_post_integration_${resourceName}`, {        type: 'AWS_PROXY',        httpMethod: apiGatewayMethod.httpMethod,        integrationHttpMethod: 'POST',        resourceId: apiGatewayResource.id,        restApiId: restApi.id,        uri: lambda.invokeArn,        credentials: credentials.arn    });    redeploymentTriggerElements.push(apiGatewayMethod.id, apiGatewayMethodIntegration.id);    } // defineRestApiResourceDeployment()    if (lambdas.create) {        defineMethodForResource(\"POST\", lambdas.create);    }    if (lambdas.read) {        defineMethodForResource(\"GET\", lambdas.create!);    }    if (lambdas.update) {        defineMethodForResource(\"PUT\", lambdas.create!);    }    if (lambdas.delete) {        defineMethodForResource(\"DELETE\", lambdas.create!);    }    const deployment = new APIGateway.ApiGatewayDeployment(stack, 'api_gateway_deployment', {        restApiId: restApi.id,        triggers: {            redeployment: Fn.sha1(Fn.jsonencode(redeploymentTriggerElements))        },        lifecycle: {            createBeforeDestroy: true        }    });    return deployment;}If we do this for all resources, our stack definition becomes much more legible (imo): class ApiGatewayTutorialStack extends TerraformStack {    constructor(scope: Construct, id: string, profile: string) {        super(scope, id)        new AwsProvider(this, 'aws', {            region: 'eu-west-1',            profile: profile        })        defineApiGatewayAccount(this);        const lambdaApiGatewayRole = defineApiGatewayRole(this);        const sourceBucket = defineSourceS3Bucket(this, 'franck-iac-lambda-source-bucket');        const sourceBucketObjectForLambdaSkeleton = defineSourceBucketObjectForLambdaSkeleton(this, sourceBucket);        const lambdaFunctionForPostMethod = defineLambdaFunctionForPostMethod(this, sourceBucket, sourceBucketObjectForLambdaSkeleton, lambdaApiGatewayRole);        const restApiCredentials = defineRestApiCredentials(this, lambdaFunctionForPostMethod);        const restApiDeployment = defineRestApiResourceDeployment(this, 'dynamodbmanager', { create: lambdaFunctionForPostMethod }, restApiCredentials);        const apiGatewayDeploymentStage = defineApigatewayDeploymentStage(this, 'v1', restApiDeployment);        // output the API URL        new TerraformOutput(this, \"invoke-url\", {            description: 'Invoke URL for the API',            value: apiGatewayDeploymentStage.invokeUrl        });    }}Eventually, these functions could be moved to a typescript module and reused across stacks. The world is my oyster. You can see the final code here and compare it to the generated HCL.My IaC user policy is here. Terraform graph I would really like a pretty picture to accompany this blog, so I am going to try terraform built in graph function. I start by installing graphviz with chocolatey choco install graphviz I can then navigate to the generated terraform files folder \\day04\\cdktf.out\\stacks\\day04 and execute &gt; terraform graph | dot -Tsvg &gt; ../../../../docs/day04.svg which gives me this day04.svg file:  Which is useful but quite ugly :frowning_face: Conclusion I can see the benefits of using the terraform CDK to generate my Infrastructure as Code files. Using a strongly typed language allows for a much more legibility and flexibility than HCL Next on my list are tags:1) use them to tighten my IAM policies2) check whether the terraform CDK allow, like the AWS one, tagging at construct level and of course actually deploying the code for my lambda, and complete the tutorial by creating a dynamoDB table to manipulate via my API.             see also this blog Benchmarking S3 force_destroy¬†&#8617;               Interestingly, unless I am horribly mistaken, the exemplar lambda in the API Gateway tutorial does not return the correct schema to be used with the API Gateway!¬†&#8617;       ","categories": [],
        "tags": ["iac","terraform","cdktf"],
        "url": "https://franck-chester.github.io//2021/11/10/Terraform-CDK-part-2.html"
      },{
        "title": "From monolithic to MACH architecture",
        "excerpt":"It‚Äôs been a while since my last proper post, but I‚Äôve given a presentation last week that, I believe, deserves to be expanded into a post. The presentation was about my current employer, the Very Group, transformation away from its legacy systems and processes,and the technology behind that modernisation. My contribution to the presentation was specifically about howthe Digital Customer Experience (DCX) tribe will re-architectour flagship e-commerce websites away from our ageing monolithic platforms towards a modern, composable, MACH architecture . You can watch the whole presentation here:  In this post, I reproduce the slides (badly, as powerpoint export to GIF function is poo), together with additionalthoughts I didn‚Äôt have time to cover during my slot. The problem of the monolith  A monolithic platform such as Oracle e‚Äëcommerce ATGwill always, sooner or later, lead to spaghetti logic. A single codebase will cause well intentioned engineersto apply DRY principles, and shared components which will eventually cause logically independent business concernsto become coupled through shared code artefacts. Eventually, on a large codebase such as the Very Group websites and underlying business systems, this coupling makes it near impossible to experiment or accelerate the deployment of new features, as every single code change risks impacting other unrelated features. Releases have to be batched, with sufficient time in betweento allow for regression testing. Automating these testsis itself difficult as each combination of features causes an explosion in the number of regression tests required. All of this is incompatible with moderns ways of developing and deployingsoftware, which privilege small and frequent featurereleases with quick customer feedback loops to adjust the direction of travelbefore prioritising the next feature. Breaking down the monolith - bit by bit  We do not have the luxury of recreating our platform from scratch.We must break the monolith down into its constituent parts before we can start replacing these, one by one, like the planks of Theseus‚Äô ship, without impacting our customers‚Äô experience. Ship of Theseus  This thought experiment from the philosophers of ancient Greece asks whether an object that has had all of its components replaced remains fundamentally the same object. Putting aside the philosophical question, we will inspireourselves from that approach and replace our monolith not through one big bang all or nothing approach, but very gradually,plank by plank, so that our customers will at all timeremain unaware of the transformation. Unfortunately, by definition, a monolith is not made ofindividual components, so, how will we identify the bits we can replace? We will slice and dice the monolith along logical boundaries that might not exist inthe codebase but are meaningful to our team. Logically distinct customer journeys  Our engineering squads are already organised along the journeys our customers take through our website.The presentation and business logic associated with let‚Äôs say product discovery(browsing for and eventually selecting a specific product)is logically distinct from customer onboarding (creating an account)or account management. These journeys are therefore an easy way to slice through the responsibilities of our monolith. Logically distinct architecture layers  Although monolithic, our platform is layered, with distinct areas of the codebase dealing with front end, business logic and integration with other systems.Again, the codebase itself will have undesirable dependencies,but these architectural layers are logically independent and another easy way to slice through the monolith. Logically distinct bounded contexts  We now look at our entire solution, or rather the problems we are solving with that solution.According to Domain Driven Design (DDD) This ‚Äòproblem domain‚Äô can be broken down into subdomains that are eithercore, generic or supporting:       The core domain is what truly differentiates us from our competitors, our bread and butter, our Unique Selling Point (USP). This is what matters to us as an organisation and where we want to put in all our efforts.     Our core problem here at the Very Group is how to provide the best digital customer experience, across multi channels,and make it as easy as possible to find and buy the right product, at the right price, with, if required,financial options, while meeting all our compliance requirements.         On the other hand, many problems in our domain are quite generic across our industry and actually quite difficult to solve in any way that would differentiate us from our competitors.     Instead, we are better off partnering with 3rd parties who are happy to treat these problems as their core domain and for us to simply consume those 3rd parties‚Äô solution to these problems.         finally, supporting sub domains are problems that we must solve in order to address our core problems, but will not in themselves differentiate us from our competitors.     Within these domains, we can now identify problem area that are specific enough from other problem areas. Given problem X, let‚Äôs say‚Äúdata associated with a customer‚Äù, we can clearly state that it belongs in the ‚Äúcustomer‚Äù problem area, and not the ‚Äúshopping cart‚Äù one.   We can draw a clear boundary around each of these problem areas, and deal with each independently from others. This is of course a gross over-simplification of what Domain Driven Design calls ‚ÄúBounded Contexts‚Äù, see Eric Evans‚Äô original book or the muchmore actionable follow-up by his colleague Vaugh Vernon for a muchbetter explanation. Strangler fig pattern  We have now sliced and diced our monolith into bounded contexts,mapped these to our customer journeys and architecture layersand decided whether they address a core, generic or supportingproblem. We now need to start reimplementing them away fromthe monolith, and introduce them into the overall solutionwithout the customer being aware. We will achieve this by leveraging the Strangler Fig pattern,and introducing fa√ßade components in front of our monolith, tasked with shunting traffic away from the monolithtoward our new components as and when these become available. As we reimplement more and more of our solution on ournew architecture, the fa√ßade will shunt more and morerequests away from the monolith, until eventually we willbe able to remove it and the fa√ßade altogether. Again this is a gross over simplification of the full decomposition process. There willbe multiple fa√ßades, between architecture layers, andwhere our customer journeys are not truly independent,each trying to expose a nice clean interface over the spaghetti logic underneath it. MACH target architecture Once we have a strategy to break up the monolith, we can target a modern, non-monolithic, composable architecture. MACH is a cute acronym coined in the retail industry to market SaaS platforms that can be composed into best-of-breed e-commerce solutions. It stands for Microservices, API-first, Cloud native SaaS and Headless components and is very much the pattern you‚Äôd use, regardless of industry, to build a modern business application Microservices  Microservices are the technical implementation of the bounded contexts we mentioned earlier: Having identified tightly defined problems areas witha clear boundary between what does and doesn‚Äôt pertain to a specific problem,we can design and implement specific solutions to these specific problems.These solutions should each be applications in their own right,managed entirely separately from each other, truly independently. Instead of one big monolithic solution to all our problems, we willhave much smaller (hence the micro- prefix) individual solutions to individual problems. NB: Yet another over-simplification of what is an architecture style in its own right. APIs and Events  To be truly independent from each other, microservices should only communicate (exchange data) via APIs or events.APIs, Application Programming Interfaces, are basically an agreed contract between a service and its clients, that defines every single detail of the communication, from the protocol, to the data exchanged, the format that data will take, how errors will be returned, etc‚Ä¶ Having agreed on that contract, both sides, service and clients, can implement it whichever way they see fit, independently from each other, using whatever technology or framework they prefer. APIs do however imply a client-service relationship whereas the client calls the service‚Äôs API and expects a response.Even an asynchronous API will follow that pattern, but broken over 2 separate exchange, one for the request from the client, which is immediately acknowledged by the service, and a later one for the response, from the service to the client, which is also acknowledged by the client. If either side is not present, or fails during the interaction, the communication will fail. Events, on the other hand, remove that runtime dependency, by totally decoupling the event publisher from its eventual consumers.Events are fired into the ether (actually a bus or hub built into the overall platform to be shared across components) without knowing, or caring as to who will consume them, or when. The 2 sides of the communication do not even need to be running at the same time : the publisher might run just long enough to fire an event and then stop, naturally or catastrophically, without it affecting the consumers. This is a very bad explanation - I need to rewrite this bit. From a MACH architecture point of view however, what matters is that APIs and Events allow microservices to be developed, deployed and managed independently of each other. Cloud native components and technologies  Cloud native can mean different things to different people, or in different contexts. On the one hand, you have the definition from the Cloud Native Computing Foundation: ‚ÄúCloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.‚Äù, and its infamous ‚Äòcloud native landscape‚Äô. On the other, we have MACH‚Äôs definition: ‚ÄúSoftware as a Service developed, designed and deployed as cloud-native applications are composed of several independent services. The independence of each service introduces the ability to maintain and scale, in isolation, horizontally rather than vertically.‚Äù. Personally, I like to think of cloud native as leveraging value-added services from the cloud, i.e. work on the right hand-side of this diagram, from SaaS to PaaS:  SaaS: 3rd party software incorporated into our architecture to provide generic domain functionality, should be consumed ‚Äòas a Service‚Äô, meaning via APIs or events, and have near zero infrastructure footprint. FaaS: the code we implement to provide core domain functionality, should be, as much as possible, be implemented as Functions, allowing us to concentrate on our core problem and leave all the ‚Äúundifferentiated heavy lifting of triggering, running and scaling (up or down) that code to our cloud provider. PaaS: the platform we deploy and run our software on should itself be accessed ‚Äòas a Service‚Äô. Storage, queues, event infrastructure, load-balancing, monitoring, alerting, security, and all those other myriad concerns, should again be managed on our behalf by the cloud provider, with as much complexity as possible entirely hidden or abstracted away from us, to again, let us concentrate on our core domain. During the Hackajob presentation, an attendee asked about multi-cloud, meaning the ability to deploy our software on any or all the available cloud vendors.It is important to note that ‚Äúmulti-cloud‚Äù is only possible if building on the left hand-side of the diagram above, limiting yourself to Infrastructure as a Service (IaaS) or Containers as a Service (CaaS) and is actually the polar opposite of leveraging value-added services. To be truly Cloud Native means working together, and trusting your cloud and SaaS provider, and believing they are much much better than you at running infrastructure at scale. Headless components and services  In order to keep control of all aspect of our Customers experience, it is essential that all the components we incorporate into our solution remain completely Headless, meaning have no user interface, and do not impose any specific user workflows. As stated above, all interactions must be ‚Äòas a Service‚Äô, i.e. via APIs or events, and these must not convey any element of user interaction, be it user messages or any sort of content formatting, and make no assumption as to the user journey that led to these interactions, or whether there is indeed an actual user involved. Technologies behind our transformation I am not actually going to expand on these much more here, give myself some material for future blogs.  SaaS partners   Amplience ‚Äì Content Management Service  Commercetools ‚Äì e-commerce engineAWS PaaS value-added services   Lambda functions ‚Äì business logic  API Gateway ‚Äì REST APIs  EventBridge ‚Äì events driven services  Step Functions ‚Äì orchestration &amp; integration  ElastiCache ‚Äì Redis &amp; Memcached  DynamoDb ‚Äì NoSQL data storageDCX technology stack   Javascript + typescript ‚Äì development language  React + fastify - web UI  Kotlin &amp; swift -  native apps  Storybook ‚Äì UI Development  Node.js ‚Äì microservices  Open API, Supertest¬†&amp; PACT ‚Äì API design &amp; testing  Lighthouse &amp; cypress ‚Äì end-to-end testing  Elastic - monitoring (logs &amp; APM &amp; RUM)  Terraform -  Infrastructure as Code  Fastlane ‚Äì app release management  Gitlab &amp; Jenkins ‚Äì CI/CD pipelines","categories": [],
        "tags": ["aws","mach","architecture","monolith"],
        "url": "https://franck-chester.github.io//2022/06/06/monolith-to-mach.html"
      }]
